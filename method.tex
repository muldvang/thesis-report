% chktex-file -1
\chapter{Method}
\label{cha:method}

\section{Viterbi as Linear Algebra}
\label{sec:algorithm-as-linear}

The above classical Viterbi algorithm can be reformulated into a number of
matrix multiplications. This is what \citet{sand2013ziphmmlib} and
\citet{lifshits2009speeding} use too. First, let $B_{o_i}$ be the diagonal
matrix, having the emission probabilities of $o_i$ of the diagonal:
\begin{equation*}
  B_{o_i} =
  \begin{bmatrix}
    b_{1, o_i} &            &        &            \\
               & b_{2, o_i} &        &            \\
               &            & \cdots &            \\
               &            &        & b_{N, o_i} \\
  \end{bmatrix}
\end{equation*}
and let
\begin{align*}
  C_{o_i} &= B_{o_i} A^* \\
  C_1 &= B_{y_1} \pi,
\end{align*}
where $A^*$ is the transpose of $A$.

Now $\omega_t$ can be computed using $C_{y_t}$ and $\omega_{t - 1}$ as
\begin{equation}
  \label{eq:2}
  \omega_t = C_{y_t} \odot \omega_{t - 1} = C_{y_t} \odot C_{y_{t-1}} \odot
  \dots \odot C_1,
\end{equation}
where $\odot$ is the max-times matrix multiplication defined as ${(A \odot
  B)}_{ij} = \max_k A_{ik} \cdot B_{kj}$.

The classical Viterbi algorithm corresponds to computing this from right to
left, but since matrix multiplication and max-times matrix multiplication is
associative the product may be computed in any order.

Backtracking is achieved in the same way as for the classical Viterbi algorithm
as described in section~\ref{sec:class-viterbi-algor}.

The space consumption and running time of this algorithm has changed, compared
to the classical Viterbi algorithm. For each symbol $o_i$ in the alphabet of
observables the corresponding matrix $B_{o_i}$ is created, thus requirering
$O(M N^2)$ space. The $C_{o_i}$ matrices require the same amount of space,
$O(M N^2)$. $C_1$ is vector, thus only requiring $O(N)$ space. If
equation~\eqref{eq:2} is evaluated from right to left it corresponds to a
series of matrix-vector multiplications resulting in a vector of size
$O(N)$. In total this requires $O(2 M N^2 + 2 N) = O(M N^2)$ space. If
backtracking is required the space consumption will be increased by the size of
the table of point that has size $O(N T)$. The total the space consumption with
backtracking enabled is $O(M N^2 + N T)$.

Likewise, the running time has changed. Creating the $B_{o_i}$ matrices takes
$O(M N^2)$ time. Computing the $C_{o_i}$ matrices takes time $O(M N^3)$ due to
the matrix multiplication. Computing $C_1$ takes $O(N^2)$ times. Again, if
equation~\eqref{eq:2} is evaluated from right to left, it will be matrix-vector
multiplication requiring $O(N^2 T)$ time. Backtracking can be done in $O(NT)$
time by using the table of pointers. In total this becomes $O(M N^2 + M N^3 +
N^2 + N^2 T + NT) = O(M N^3 + N^2 T)$.

\section{Exploiting Repetitions}
\label{sec:expl-repet}

In this section it is shown how to exploit repetitions in the observed sequence
to make the above algorithm run faster. \citet{lifshits2009speeding} introduces
five different methods of doing this, while \citet{sand2013ziphmmlib} only uses
byte-pair encoding.

\subsection{Byte-Pair Encoding}
\label{sec:byte-pair-encoding}

Byte-pair encoding is a simple data compression method. The most common pair
consecutive bytes in the data is replaced by a byte that do not exist in the
data. This is repeated until either all new bytes are used or the most common
pair do not appear frequently in the data. The use of bytes may easily be
replaced by used of characters or intergers. For example, the input sequence
01012012 would first be encoded into 33232 by substituting 01 with 3; in the
next iteration it would be encoded into 344 by substituding 32 by 4 etc.

\subsection{Using Byte-Pair Encoding to Speed Up Viterbi}
\label{sec:using-byte-pair}

The Viterbi algorithm achieves a speed up using byte-pair encoding in the
following way. Let $o_i o_j \in O \times O$ be the most frequently occuring
pair of symbols in $Y_{1:T}$ and let $n_{o_i o_j}$ be the number of
occurences. $o_i o_j$ is substituded by a new symbol $o_{M + 1}$ and the length
of $Y_{1:T}$ is thereby reduced by $n_{o_i o_j}$. All $n_{o_i o_j}$ occurences
of $C_{o_i} \odot C_{o_j}$ in equation~\eqref{eq:2} may be replaced by the new
matrix
\begin{equation}
  C_{o_{M + 1}} = C_{o_i} \odot C_{o_j}.
\end{equation}
Hence, the number of matrix multiplications is reduced by $n_{o_i o_j}$. The
byte-pair encoding continues until $n_{o_i o_j}$ becomes too small to give a
speed up. \fxwarning{When is this?} The result of this is a new sequence
$Y_{1:T'}'$ over the new alphabet
$\mathcal{O}' = \{o_1, o_2, \dots, o_M, o_{M + 1} = (l_1, r_1), o_{M + 2} =
(l_2, r_2), \dots, o_{M'} = (l_{M' - M}, r_{M' - M}) \}$,
where $l_i, r_i \in \{ o_1, o_2, \dots, o_{i - 1} \}$.

This method can be split up into two. First the preprocessing consisting the of
encoding the sequence as just discussed is done. As the encoding of the
sequence is independent of the HMM the encoded sequence may be saved to the
disk for later use. The second part consist of the actual Viterbi algorithm
and can be split into two stages. The first stage is the computation of
$C_{o_i}$ for $i = 1, \dots, M$ and then $C_{o_i}$ for increasing
$i = M + 1, \dots, M'$ by $C_{o_i} = C_{l_i} \odot C_{l_r}$. In the second
stage $\omega_T$ is computed by
\begin{equation}
  \label{eq:3}
  \omega_T = C_{y'_{T'}} \odot C_{y'_{T'-1}} \odot \dots \odot C_{y'_2} \odot C_1.
\end{equation}

\subsubsection{Backtracking}
\label{sec:backtracking}

Obtaining the Viterbi path $S = s_1, s_2, \dots, s_T$ of $Y_{1:T}$ is no longer
as simple as for the classical Viterbi algorithm. If the entire $\omega$ table
is stored, only the optimal Viterbi path $S' = s_1', s_2', \dots, s_{T'}'$ of
the compressed sequence $Y'_{1:T'}$ may be found using backtracking. However,
it turns out that $S$ can be inferred from $S'$ since $S'$ is a subset of $S$
as described by \citet{lifshits2009speeding}. To do this a set of matrices
$R_{o_i}$ is kept along the $C_{o_i}$ matrices for each of the new symbols
$o_{M + 1}, \dots, o_{M' - M}$ defined as
\begin{equation*}
  R_{o_{M + i}}(\text{row}, \text{col}) = \argmax_k
  \left(
    C_{l_i}(\text{row}, k) \odot C_{r_i}(k, \text{col})
  \right).
\end{equation*}
This is looks like the definition of the $C_{o_i}$ matrices, but instead of
storing the maximum value, the state that results in the maximum value is
stored.

Now, for each occurence of a new symbol $o_{M + i} = (l_i, r_i)$ where
$l_i, r_i \in \mathcal{O}$ in $Y_{1:T'}'$ we know the start state $s_l$ and the
end state $s_r$ from $S'$, such that $s_l$ is the state immediatly before $l_i$
was emitted and $s_r$ is the state after $r_i$ was emitted. Hence, we need to
find the most likely state where $l_i$ is emitted. This state is easily
obtained since it is stored at $R_{o_{M + i}}(s_l, s_r)$. In the case where one
or both of $l_i, r_i \not \in \mathcal{O}$ we can apply this method
recursively.

\subsubsection{Running Time}
\label{sec:running-time}

The space consumption is comparable to the Viterbi algorithm without
compression enabled. The number of $C$ matrices has changed from $M$ to $M'$
thus requiring $O(M' N^2)$ space. The introduction of the $R$ matrices does not
change this since the $C$ matrices are of similar size. The table used for
backtracking has decreased to size $O(N T')$ yielding a total space consumption
of $O(M' N^2 + N T')$.

With similar arguments the running time has changed to $O(M' N^3 +N^2 T')$.

\subsubsection{Saving Computed Stuff}
\label{sec:saving-comp-stuff}
\fxwarning{Better title.}

As mentioned the sequence encoding is independent of the HMM and may be saved
for use with different HMMs. \citet{lifshits2009speeding} also suggests
constructing the substitution table offline based on a set of representative
sequences. Then the $C_{o_i}$ matrices could be computed beforehand and also
saved to the disk.

\subsection{Numerical Stability}
\label{sec:numerical-stability}

All matrices contain probabilities, i.e.\ the are in range between 0 and
1. Multiplying them together results in even smaller numbers. Since the value
is stored as a IEEE 754 floating point format there is a limited precission,
and the computations will quickly underflow. \citet{sand2013ziphmmlib}
describes how to avoid underflow in terms to the forward algorithm. It turns
out that it is easier to avoid for the Viterbi algorithm.

To circumvent underflow in the classical Viterbi algorithm all probabilities
are converted to log-space. So, instead of computing $\omega_T$,
$\log \omega_T$ is computed. By using the rules of logarithms the
multiplications are turned into additions using the rule
$\log(AB) = \log A + \log B$. \fxnote{Insert some reference and elaborate a
  bit.}

The same idea can be used for the matrix based approach, where
equation~\eqref{eq:3} is rewritten as
\begin{align*}
  \log \omega_T &= \log \left(C_{y'_{T'}} \odot C_{y'_{T'-1}} \odot \dots \odot
                  C_{y'_2} \odot C_1 \right) \\
                &= \log C_{y'_{T'}} \oplus \log C_{y'_{T'-1}} \oplus \dots \oplus
                  \log C_{y'_2} \oplus \log C_1,
\end{align*}
and the $C$ matrices are rewritten as
\begin{align*}
  C_1 &= \log B_{y_1} \pi, \\
  C_{o_i} &= \log B_{o_i} A^*, \quad \text{for }1 \le i \le M\\
  C_{o_{M + i}} &= C_{l_i} \oplus C_{r_i} , \quad \text{for }1 \le i \le M' - M
\end{align*}
where $\oplus$ is defined as
${ \left( A \oplus B \right)}_{ij} = \max_k \left( \log A_{ik} + \log B_{kj}
\right)$.


\subsection{Compression Stopping Criterion}
\label{sec:compr-stopp-crit}

\fxnote{This is exactly as in \citet{sand2013ziphmmlib}.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
