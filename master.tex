% chktex-file 24
\documentclass[11pt,twoside,a4,danish,english,report]{memoir}

% Math
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

% Font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{pxfonts}
\usepackage{microtype}

% Citations
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

% Graphics
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{pgfplots.colorbrewer}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz/,mode=list and make]
\pgfplotsset{%
  colorbrewer cycle list=Set1,
  legend style={at={(1,0.5)},anchor=west,draw=none},
  legend cell align=left
  % legend columns=5,
  % /tikz/column 2/.style={
  % column sep=10pt,
  % },
  %   /tikz/column 4/.style={
  %   column sep=10pt,
  % },
  %   /tikz/column 6/.style={
  %   column sep=10pt,
  % },
  %   /tikz/column 8/.style={
  %   column sep=10pt,
  % }
}

% Date
\usepackage{datetime}

% Captions
\usepackage[hang,bf]{caption}

% Links
\usepackage[hidelinks]{hyperref}

% Fixme
\usepackage[draft]{fixme}

% User defined macros
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\frontmatter{}

\begin{titlingpage}
  \vspace*{\fill}

  \fxnote{Use font size 10pt}
  \noindent{\rule{\linewidth}{1mm}\\[4ex]
    {\Huge\sffamily Speeding Up HMM Decoding Using Compression}\\[2ex]
    {\huge\sffamily Torben Muldvang Andersen, 20093713}\\[2ex]
    \rule{\linewidth}{1mm}\\[4ex]
    {\Large\sffamily
      Master's Thesis, Computer Science\\[1ex]
      \monthname\ \the\year\\[1ex]
      Adviser: Christian N. S. Pedersen\\[15ex]}\\[\fill]
    \includegraphics{logo}
  }
\end{titlingpage}

\listoffixmes{}

\chapter{Abstract}
\label{cha:abstract}

in English\dots

\selectlanguage{danish}
\chapter{Resum√©}
\label{cha:resume}

in Danish\dots
\selectlanguage{english}

\chapter{Acknowledgements}
\label{cha:acknowledgements}

\vspace{2ex}
\begin{flushright}
  \emph{Torben Muldvang Andersen,}\\
  \emph{Aarhus, \today.}
\end{flushright}

\cleardoublepage{}

\tableofcontents

\mainmatter{}

\chapter{Introduction}
\label{cha:introduction}

\begin{itemize}
\item Problemformulering / hypotese
\item Metode og overblik
\end{itemize}

\chapter{Background and Related Work}
\label{cha:backgr-relat-work}

\fxfatal{Something about HMMs in general.}

\citet{lifshits2009speeding} present a method for speeding up the dynamic
programming algorithms used with HMMs, namely the forward-backward algorithms
and the Viterbi algorithm. The approach is based on finding repeated substrings
in the observed input sequence. These substrings are found using five different
algorithms: Four Russians method, run length encoding, Lempel-Ziv parsing,
grammer-based compression and byte pair encoding. The forward-backward and the
Viterbi algorithms are the reformulated into series of matrix
multiplications. The overall idea is that the repeated substrings correspond to
repeated matrix multiplications and by finding the repeated substrings the
multiplications can be avoided. Unfortunately, the code has not been made
public available and the number of experiments and the quality of these is
quite limited.

\citet{sand2013ziphmmlib} present zipHMMlib, a highly optimized HMM library for
speeding up the forward algorithm. Much of the theory in this paper rely on
\cite{lifshits2009speeding}, but it is extended to make the computations
numerically stable. Furthermore, the code is available as an open source
library with bindings for both Python and R.

This thesis extends the work made by \citet{sand2013ziphmmlib} to also include
a highly efficient Viterbi algorithm based on the theory developed by
\citet{lifshits2009speeding}, but as \citet{sand2013ziphmmlib} the theory will
also be extended a bit to make the computation numerically stable. Furthermore,
the experiments will be more extensive, thus exploiting some cases in which
the original formulation of the algorithm will be more efficient.

\chapter{Preliminaries}
\label{cha:preliminaries}

In the section the definition of and notation for a Hidden Markov Model and the
Viterbi algorithm will be presented using the notation also used by
\citet{sand2013ziphmmlib}.

\section{Hidden Markov Models}
\label{sec:hidden-markov-models}

A hidden Markov model is a statistical model in which it is assumed that an
observed sequence is generated by a Markov process with unobserved hidden
states. Hence, for a sequence $Y_{1:T} = y_1y_2\dots{}y_T \in \mathcal{O*}$
generated by the model there exist one or more hidden sequences
$X_{1:T} = x_1x_2\dots{}x_T \in \mathcal{H*}$, with $\mathcal{O}$ and
$\mathcal{H}$ being finite alphabets over the observables and hidden
states. The hidden sequence may be seen as a explanation of the observed
sequence.

Formally a HMM can be defined as
\begin{itemize}
\item $\mathcal{H} = {h_1, h_2, \dots, h_N}$, a finite alphabet of hidden
  states;
\item $\mathcal{O} = {o_1, o_2, \dots, o_M}$, a finite alphabet of observables;
\item a vector $\Pi = {(\pi_i)}_{1 \le i \le N}$, where $\pi_i = \Pr(x_1 =
  h_i)$ is the probablity of the model starting in hidden state $h_i$;
\item a matrix $A = {\{a_{ij}\}}_{1 \le i \le N}$, where $a_{ij} = \Pr(x_t
  = h_j \mid x_{t - 1} = h_i)$ is the probability of a transition from state
  $h_i$ to state $h_j$;
\item a matrix $B = {\{b_{ij}\}}_{1 \le i \le N}^{1 \le j \le M}$, where
  $b_{ij} = \Pr(y_t = o_j \mid x_t = h_i)$ is the probability of state
  $h_i$ emitting $o_j$.
\end{itemize}

An HMM is parameterised by $\pi$, $A$, and $B$, which is denoted by $\lambda =
(\pi, A, B)$.

\section{The Classical Viterbi Algorithm}
\label{sec:class-viterbi-algor}

The Viterbi algorithm finds the probability of the most likely sequence of
hidden states given a model $\lambda$ and an observed sequence $Y_{1:T}$ by
maximizing the probability of the observed and hidden sequences for all
possible hidden sequences: $\Pr(Y_{1:T} \mid \lambda) = \max_{x_{1:T}}
\Pr(Y_{1:T}, X_{1:T} = x_{1:T} \mid \lambda)$. This may be computed efficiently
by filling out a table, $\omega$, with entries $\omega_t(x_t) = \Pr(Y_{1:T},
X_t = x_t \mid \lambda) = \max_{x_{1:t-1}} \Pr(Y_{1:t}, X_{1:t} = x_{1:t} \mid
\lambda)$ column by column from left to right, using the recursion
\begin{align*}
  \omega_1(x_1) &= \pi_{x_1} b_{x_1, y_1} \\
  \omega_t(x_t) &= b_{x_t, y_t} \max_{x_{t - 1}} \omega_{t - 1}(x_{t - 1})
                  a_{x_{t - 1}, x_t}.
\end{align*}
After filling out $\omega$, $\Pr(Y_{1:T} \mid \lambda)$ can be computed as
$\Pr(Y_{1:T} \mid \lambda) = \max_{x_T} \omega_T(x_T)$.

To obtain the sequence of hidden states $\omega$ is backtracked using the
recursion
\begin{align*}
  X_T &= \argmax_{x_T} \omega_T(x_T) \\
  X_{t} &= \argmax_{x_{t}} b_{X_{t + 1}, y_{t + 1}} \omega_{t}(x_t) a_{x_t, X_{t + 1}}.
\end{align*}

The space consumption of this algorithm is the size of $\omega$ which is $O(N
T)$. The time required to fill out a cell, the algorithm maximizes over all
cells in the previous column yielding a running time of $O(N^2 T)$.

\chapter{Method}
\label{cha:method}

\section{Viterbi as Linear Algebra}
\label{sec:algorithm-as-linear}

The above classical Viterbi algorithm can be reformulated into a number of
matrix multiplications. This is what \citet{sand2013ziphmmlib} and
\citet{lifshits2009speeding} use too. First, let $B_{o_i}$ be the diagonal
matrix, having the emission probabilities of $o_i$ of the diagonal:
\begin{equation*}
  B_{o_i} =
  \begin{bmatrix}
    b_{1, o_i} &            &        &            \\
               & b_{2, o_i} &        &            \\
               &            & \cdots &            \\
               &            &        & b_{N, o_i} \\
  \end{bmatrix}
\end{equation*}
and let
\begin{align*}
  C_{o_i} &= B_{o_i} A^* \\
  C_1 &= B_{y_1} \pi,
\end{align*}
where $A^*$ is the transpose of $A$.

Now $\omega_t$ can be computed using $C_{y_t}$ and $\omega_{t - 1}$ as
\begin{equation}
  \label{eq:2}
  \omega_t = C_{y_t} \odot \omega_{t - 1} = C_{y_t} \odot C_{y_{t-1}} \odot
  \dots \odot C_1,
\end{equation}
where $\odot$ is the max-times matrix multiplication defined as ${(A \odot
  B)}_{ij} = \max_k A_{ik} \cdot B_{kj}$.

The classical Viterbi algorithm corresponds to computing this from right to
left, but since matrix multiplication and max-times matrix multiplication is
associative the product may be computed in any order.

\fxerror{How to backtrack?}

The space consumption and running time of this algorithm has changed, compared
to the classical Viterbi algorithm. For each symbol $o_i$ in the alphabet of
observables the corresponding matrix $B_{o_i}$ is created, thus requirering
$O(M N^2)$ space. The $C_{o_i}$ matrices require the same amount of space,
$O(M N^2)$. $C_1$ is vector, thus only requiring $O(N)$ space. If
equation~\eqref{eq:2} is evaluated from right to left it corresponds to a
series of matrix-vector multiplications resulting in a vector of size
$O(N)$. In total this requires $O(2 M N^2 + 2 N) = O(M N^2)$ space. If
backtracking is required the space consumption will be increased by the size of
the table of point that has size $O(N T)$. The total the space consumption with
backtracking enabled is $O(M N^2 + N T)$.

Likewise, the running time has changed. Creating the $B_{o_i}$ matrices takes
$O(M N^2)$ time. Computing the $C_{o_i}$ matrices takes time $O(M N^3)$ due to
the matrix multiplication. Computing $C_1$ takes $O(N^2)$ times. Again, if
equation~\eqref{eq:2} is evaluated from right to left, it will be matrix-vector
multiplication requiring $O(N^2 T)$ time. Backtracking can be done in $O(NT)$
time by using the table of pointers. In total this becomes $O(M N^2 + M N^3 +
N^2 + N^2 T + NT) = O(M N^3 + N^2 T)$.

\section{Exploiting Repetitions}
\label{sec:expl-repet}

In this section it is shown how to exploit repetitions in the observed sequence
to make the above algorithm run faster. \citet{lifshits2009speeding} introduces
five different methods of doing this, while \citet{sand2013ziphmmlib} only uses
byte-pair encoding.

\subsection{Byte-Pair Encoding}
\label{sec:byte-pair-encoding}

Byte-pair encoding is a simple data compression method. The most common pair
consecutive bytes in the data is replaced by a byte that do not exist in the
data. This is repeated until either all new bytes are used or the most common
pair do not appear frequently in the data. The use of bytes may easily be
replaced by used of characters or intergers. For example, the input sequence
01012012 would first be encoded into 33232 by substituting 01 with 3; in the
next iteration it would be encoded into 344 by substituding 32 by 4 etc.

\subsection{Using Byte-Pair Encoding to Speed Up Viterbi}
\label{sec:using-byte-pair}

The Viterbi algorithm achieves a speed up using byte-pair encoding in the
following way. Let $o_i o_j \in O \times O$ be the most frequently occuring
pair of symbols in $Y_{1:T}$ and let $n_{o_i o_j}$ be the number of
occurences. $o_i o_j$ is substituded by a new symbol $o_{M + 1}$ and the length
of $Y_{1:T}$ is thereby reduced by $n_{o_i o_j}$. All $n_{o_i o_j}$ occurences
of $C_{o_i} \odot C_{o_j}$ in equation~\eqref{eq:2} may be replaced by the new
matrix
\begin{equation}
  C_{o_{M + 1}} = C_{o_i} \odot C_{o_j}.
\end{equation}
Hence, the number of matrix multiplications is reduced by $n_{o_i o_j}$. The
byte-pair encoding continues until $n_{o_i o_j}$ becomes too small to give a
speed up. \fxwarning{When is this?} The result of this is a new sequence
$Y_{1:T'}'$ over the new alphabet
$\mathcal{O}' = \{o_1, o_2, \dots, o_M, o_{M + 1} = (l_1, r_1), o_{M + 2} =
(l_2, r_2), \dots, o_{M'} = (l_{M' - M}, r_{M' - M}) \}$,
where $l_i, r_i \in \{ o_1, o_2, \dots, o_{i - 1} \}$.

This method can be split up into two. First the preprocessing consisting the of
encoding the sequence as just discussed is done. As the encoding of the
sequence is independent of the HMM the encoded sequence may be saved to the
disk for later use. The second part consist of the actual Viterbi algorithm
and can be split into two stages. The first stage is the computation of
$C_{o_i}$ for $i = 1, \dots, M$ and then $C_{o_i}$ for increasing
$i = M + 1, \dots, M'$ by $C_{o_i} = C_{l_i} \odot C_{l_r}$. In the second
stage $\omega_T$ is computed by
\begin{equation}
  \label{eq:3}
  \omega_T = C_{y'_{T'}} \odot C_{y'_{T'-1}} \odot \dots \odot C_{y'_2} \odot C_1.
\end{equation}

\subsubsection{Saving Computed Stuff}
\label{sec:saving-comp-stuff}
\fxwarning{Better title.}

As mentioned the sequence encoding is independent of the HMM and may be saved
for use with different HMMs. \citet{lifshits2009speeding} also suggests
constructing the substitution table offline based on a set of representative
sequences. Then the $C_{o_i}$ matrices could be computed beforehand and also
saved to the disk.

\subsection{Numerical Stability}
\label{sec:numerical-stability}

All matrices contain probabilities, i.e.\ the are in range between 0 and
1. Multiplying them together results in even smaller numbers. Since the value
is stored as a IEEE 754 floating point format there is a limited precission,
and the computations will quickly underflow. \citet{sand2013ziphmmlib}
describes how to avoid underflow in terms to the forward algorithm. It turns
out that it is easier to avoid for the Viterbi algorithm.

To circumvent underflow in the classical Viterbi algorithm all probabilities
are converted to log-space. So, instead of computing $\omega_T$,
$\log \omega_T$ is computed. By using the rules of logarithms the
multiplications are turned into additions using the rule
$\log(AB) = \log A + \log B$. \fxnote{Insert some reference and elaborate a
  bit.}

The same idea can be used for the matrix based approach, where
equation~\eqref{eq:3} is rewritten as
\begin{align*}
  \log \omega_T &= \log (C_{y'_{T'}} \odot C_{y'_{T'-1}} \odot \dots \odot
                  C_{y'_2} \odot C_1) \\
                &= \log C_{y'_{T'}} \oplus \log C_{y'_{T'-1}} \oplus \dots \oplus
                  \log C_{y'_2} \oplus \log C_1,
\end{align*}
and the $C$ matrices are rewritten as
\begin{align*}
  C_1 &= \log B_{y_1} \pi, \\
  C_{o_i} &= \log B_{o_i} A^*, \quad \text{for }1 \le i \le M\\
  C_{o_{M + i}} &= C_{l_i} \oplus C_{r_i} , \quad \text{for }1 \le i \le M' - M
\end{align*}
where $\oplus$ is defined as
${(A \oplus B)}_{ij} = \max_k \log A_{ik} + \log B_{kj}$.


\subsection{Compression Stopping Criterion}
\label{sec:compr-stopp-crit}

\fxnote{This is exactly as in \citet{sand2013ziphmmlib}.}

\chapter{Implementation}
\label{cha:implementation}

\chapter{Experiments}
\label{cha:experiments}

\section{Assymptotic Running Times}
\label{sec:assymp-runn-times}

\subsection{Sequence Length $n$}
\label{sec:sequence-length-n}

\begin{figure}[H]
  \centering
  \input{figures/pre_viterbi_n}
  \caption{zipHMMlib preprocessing time for varying sequence lengths.}
  \label{fig:pre_viterbi_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_n}
  \caption{zipHMMlib running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_path_n}
  \caption{zipHMMlib Path running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_path_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_backtrack_n}
  \caption{zipHMMlib Path backtracking time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_backtrack_n}
\end{figure}

\subsection{Model Size $k$}
\label{sec:model-size-k}

\begin{figure}[H]
  \centering
  \input{figures/pre_viterbi_k}
  \caption{zipHMMlib preprocessing time for varying model sizes.}
  \label{fig:pre_viterbi_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_k}
  \caption{zipHMMlib running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_path_k}
  \caption{zipHMMlib Path running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_path_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_backtrack_k}
  \caption{zipHMMlib Path backtracking time for varying model sizes.}
  \label{fig:assymptotic_viterbi_backtrack_k}
\end{figure}

\section{Comparing zipHMMlib to the original Viterbi algorithm.}
\label{sec:comp-ziphmml-orig}

\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_complexity.tex}
  \caption{Running time vs.\ sequence complexity using sequences of length $10^6$.}
  \label{fig:speedup_vs_complexity}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_k.tex}
  \caption{Running time vs.\ model size.}
  \label{fig:speedup_vs_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_sequence_length.tex}
  \caption{Running time vs.\ sequence length using a HMM with 16 states.}
  \label{fig:speedup_vs_sequence_length}
\end{figure}

\chapter{Conclusion}
\label{cha:conclusion}

\appendix{}

\chapter{Some Appendix}
\label{cha:some-appendix}

\backmatter{}

\bibliography{master}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
