% chktex-file 24
\documentclass[11pt,twoside,a4,danish,english,report]{memoir}

% Math
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

% Font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{pxfonts}
\usepackage{microtype}

% Citations
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

% Graphics
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{pgfplots.colorbrewer}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz/,mode=list and make]
\pgfplotsset{%
  colorbrewer cycle list=Set1,
  legend style={at={(1,0.5)},anchor=west,draw=none},
  legend cell align=left
  % legend columns=5,
  % /tikz/column 2/.style={
  % column sep=10pt,
  % },
  %   /tikz/column 4/.style={
  %   column sep=10pt,
  % },
  %   /tikz/column 6/.style={
  %   column sep=10pt,
  % },
  %   /tikz/column 8/.style={
  %   column sep=10pt,
  % }
}

% Date
\usepackage{datetime}

% Captions
\usepackage[hang,bf]{caption}

% Links
\usepackage[hidelinks]{hyperref}

% Fixme
\usepackage[draft]{fixme}

% User defined macros
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\frontmatter{}

\begin{titlingpage}
  \vspace*{\fill}

  \noindent{\rule{\linewidth}{1mm}\\[4ex]
    {\Huge\sffamily Something about Computer Science}\\[2ex]
    {\huge\sffamily Torben Muldvang Andersen, 20093713}\\[2ex]
    \rule{\linewidth}{1mm}\\[4ex]
    {\Large\sffamily
      Master's Thesis, Computer Science\\[1ex]
      \monthname\ \the\year\\[1ex]
      Adviser: Christian N. S. Pedersen\\[15ex]}\\[\fill]
    \includegraphics{logo}
  }
\end{titlingpage}

\listoffixmes{}

\chapter{Abstract}
\label{cha:abstract}

in English\dots

\selectlanguage{danish}
\chapter{Resum√©}
\label{cha:resume}

in Danish\dots
\selectlanguage{english}

\chapter{Acknowledgements}
\label{cha:acknowledgements}

\vspace{2ex}
\begin{flushright}
  \emph{Torben Muldvang Andersen,}\\
  \emph{Aarhus, \today.}
\end{flushright}

\cleardoublepage{}

\tableofcontents

\mainmatter{}

\chapter{Introduction}
\label{cha:introduction}

\begin{itemize}
\item Problemformulering / hypotese
\item Metode og overblik
\end{itemize}

\chapter{Background and Related Work}
\label{cha:backgr-relat-work}

\fxfatal{Something about HMMs in general.}

\citet{lifshits2009speeding} present a method for speeding up the dynamic
programming algorithms used with HMMs, namely the forward-backward algorithms
and the Viterbi algorithm. The approach is based on finding repeated substrings
in the observed input sequence. These substrings are found using five different
algorithms: Four Russians method, run length encoding, Lempel-Ziv parsing,
grammer-based compression and byte pair encoding. The forward-backward and the
Viterbi algorithms are the reformulated into series of matrix
multiplications. The overall idea is that the repeated substrings correspond to
repeated matrix multiplications and by finding the repeated substrings the
multiplications can be avoided. Unfortunately, the code has not been made
public available and the number of experiments and the quality of these is
quite limited.

\citet{sand2013ziphmmlib} present zipHMMlib, a highly optimized HMM library for
speeding up the forward algorithm. Much of the theory in this paper rely on
\cite{lifshits2009speeding}, but it is extended to make the computations
numerically stable. They also introduce the idea of saving the preprocessing to
the disk for later use. Furthermore, the code is available as an open source
library with bindings for both Python and R.

This thesis extends the work made by \citet{sand2013ziphmmlib} to also include
a highly efficient Viterbi algorithm based on the theory developed by
\citet{lifshits2009speeding}, but as \citet{sand2013ziphmmlib} the theory will
also be extended a bit to make the computation numerically stable. Furthermore,
the experiments will be more extensive, thus exploiting some cases in which
the original formulation of the algorithm will be more efficient.

\chapter{Preliminaries}
\label{cha:preliminaries}

In the section the definition of and notation for a Hidden Markov Model and the
Viterbi algorithm will be presented using the notation also used by
\citet{sand2013ziphmmlib}.

\section{Hidden Markov Models}
\label{sec:hidden-markov-models}

A hidden Markov model is a statistical model in which it is assumed that an
observed sequence is generated by a Markov process with unobserved hidden
states. Hence, for a sequence $Y_{1:T} = y_1y_2\dots{}y_T \in \mathcal{O*}$
generated by the model there exist one or more hidden sequences
$X_{1:T} = x_1x_2\dots{}x_T \in \mathcal{H*}$, with $\mathcal{O}$ and
$\mathcal{H}$ being finite alphabets over the observables and hidden
states. The hidden sequence may be seen as a explanation of the observed
sequence.

Formally a HMM can be defined as
\begin{itemize}
\item $\mathcal{H} = {h_1, h_2, \dots, h_N}$, a finite alphabet of hidden
  states;
\item $\mathcal{O} = {o_1, o_2, \dots, o_M}$, a finite alphabet of observables;
\item a vector $Pi = {(\pi_i)}_{1 \le i \le N}$, where $\pi_i = \Pr(x_1 =
  h_i)$ is the probablity of the model starting in hidden state $h_i$;
\item a matrix $A = {\{a_{ij}\}}_{1 \le i \le N}$, where $a_{ij} = \Pr(x_t
  = h_j \mid x_{t - 1} = h_i)$ is the probability of a transition from state
  $h_i$ to state $h_j$;
\item a matrix $B = {\{b_{ij}\}}_{1 \le i \le N}^{1 \le j \le M}$, where
  $b_{ij} = \Pr(y_t = o_j \mid x_t = h_i)$ is the probability of state
  $h_i$ emitting $o_j$.
\end{itemize}

An HMM is parameterised by $\pi$, $A$, and $B$, which is denoted by $\lambda =
(\pi, A, B)$.

\section{The Classical Viterbi Algorithm}
\label{sec:class-viterbi-algor}

The Viterbi algorithm finds the probability of the most likely sequence of
hidden states given a model $\lambda$ and an observed sequence $Y_{1:T}$ by
maximizing the probability of the observed and hidden sequences for all
possible hidden sequences: $\Pr(Y_{1:T} \mid \lambda) = \max_{x_{1:T}}
\Pr(Y_{1:T}, X_{1:T} = x_{1:T} \mid \lambda)$. This may be computed efficiently
by filling out a table, $\omega$, with entries $\omega_t(x_t) = \Pr(Y_{1:T},
X_t = x_t \mid \lambda) = \max_{x_{1:t-1}} \Pr(Y_{1:t}, X_{1:t} = x_{1:t} \mid
\lambda)$ column by column from left to right, using the recursion
\begin{align*}
  \omega_1(x_1) &= \pi_{x_1} b_{x_1, y_1} \\
  \omega_t(x_t) &= b_{x_t, y_t} \max_{x_{t - 1}} \omega_{t - 1}(x_{t - 1})
                  a_{x_{t - 1}, x_t}.
\end{align*}
After filling out $\omega$, $\Pr(Y_{1:T} \mid \lambda)$ can be computed as
$\Pr(Y_{1:T} \mid \lambda) = \max_{x_T} \omega_T(x_T)$.

To obtain the sequence of hidden states $\omega$ is backtracked using the
recursion
\begin{align*}
  X_T &= \argmax_{x_T} \omega_T(x_T) \\
  X_{t} &= \argmax_{x_{t}} b_{X_{t + 1}, y_{t + 1}} \omega_{t}(x_t) a_{x_t, X_{t + 1}}.
\end{align*}

The space consumption of this algorithm is the size of $\omega$ which is $O(N
T)$. The time required to fill out a cell, the algorithm maximizes over all
cells in the previous column yielding a running time of $O(N^2 T)$.
\chapter{Method}
\label{cha:method}

\chapter{Implementation}
\label{cha:implementation}

\chapter{Experiments}
\label{cha:experiments}

\section{Assymptotic Running Times}
\label{sec:assymp-runn-times}

\subsection{Sequence Length $n$}
\label{sec:sequence-length-n}

\begin{figure}[H]
  \centering
  \input{figures/pre_viterbi_n}
  \caption{zipHMMlib preprocessing time for varying sequence lengths.}
  \label{fig:pre_viterbi_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_n}
  \caption{zipHMMlib running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_path_n}
  \caption{zipHMMlib Path running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_path_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_backtrack_n}
  \caption{zipHMMlib Path backtracking time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_backtrack_n}
\end{figure}

\subsection{Model Size $k$}
\label{sec:model-size-k}

\begin{figure}[H]
  \centering
  \input{figures/pre_viterbi_k}
  \caption{zipHMMlib preprocessing time for varying model sizes.}
  \label{fig:pre_viterbi_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_k}
  \caption{zipHMMlib running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_path_k}
  \caption{zipHMMlib Path running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_path_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_backtrack_k}
  \caption{zipHMMlib Path backtracking time for varying model sizes.}
  \label{fig:assymptotic_viterbi_backtrack_k}
\end{figure}

\section{Comparing zipHMMlib to the original Viterbi algorithm.}
\label{sec:comp-ziphmml-orig}

\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_complexity.tex}
  \caption{Running time vs.\ sequence complexity using sequences of length $10^6$.}
  \label{fig:speedup_vs_complexity}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_k.tex}
  \caption{Running time vs.\ model size.}
  \label{fig:speedup_vs_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_sequence_length.tex}
  \caption{Running time vs.\ sequence length using a HMM with 16 states.}
  \label{fig:speedup_vs_sequence_length}
\end{figure}

\chapter{Conclusion}
\label{cha:conclusion}

\appendix{}

\chapter{Some Appendix}
\label{cha:some-appendix}

\backmatter{}

\bibliography{master}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
