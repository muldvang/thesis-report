% chktex-file 9
% chktex-file 24
% chktex-file 44
\chapter{Experiments}
\label{cha:experiments}

This chapter covers description and discussion of all experiments performed in
this thesis. First, the used data is described, followed by the experimental
setup, and then the Viterbi and posterior algorithms are discussed in turn.

\section{Data}

Hidden Markov models have successfully been applied to various kinds of
data.  As this thesis is written with no specific kind of data in mind, aside
from repetitive data of course, the experiments use random data that does not
contain many repetitions and Fibonacci words that is a highly repetitive kind
of sequence.  The two types of sequences used is now described in turn.

\begin{description}
\item[Random sequences] are generated by one of the models (described below) by
  making random transitions and emissions according to the probabilities
  specified by the parameters of that model.  The generated sequences varies in
  length from $10^3$ to approximately $10^7$.
\item[Fibonacci words] are generated in a way very similar to how Fibonacci
  numbers are generated by concatenating the two previous words, corresponding
  to generating the next Fibonacci number from the two previous ones. Let $S_0$
  be ``0'' and $S_1$ be ``01''. Now define $S_n=S_{n-1}S_{n-2}$. In contrast to
  the random sequences, Fibonacci words contain many repetitions due to the
  recursive definition and compress very well using byte-pair encoding.
\end{description}

The models used are random, fully connected models, that is there is a
transition from a state to any other state. The emission probabilities are
random too. The alphabet size has been kept constant at four. In general, the
larger the alphabet, the worse compression ratio is expected. The size of the
models, i.e.\ the number of states, have been varied from 2 to 512.

\section{Experimental Setup}

The experiments have been run on a PC with an Intel Xeon W3550 $3.07$ GHz CPU
and 4 GB RAM running GNU/Linux 3.13. As zipHMM uses the ATLAS as BLAS
implementation as default, this has also been used for the experiment. The most
current stable version, 3.10.2, was used.

Each point in the plots in this chapter corresponds to the mean of multiple
runs. For all experiments, the executions of a program with some parameters
have been repeated five times and the mean computed to even out fluctuations in
the running time. Furthermore, in the case of random sequences, 20 random
sequences of the same length has been generated and the mean of the measure
calculated, since especially for short sequences the compression ratio may vary
a lot. Furthermore, the standard deviation has been calculated and is shown as
error bars in the plots to visually indicate the fluctuations in running time.

\section{Compression Ratio} \fxnote{Make experiment with both 1 and 500?}
\label{sec:compression-ratio}

The running time of the Viterbi algorithm and the indexed posterior decoding
algorithm implemented in zipHMMlib is highly \fxnote{zipHMM or zipHMMlib?}
dependent on the size of the compressed sequence. As mentioned in
section~\ref{sec:compr-stopp-crit} it is possible to compress the sequence to a
single character, even though this may not be a good idea in terms of
performance. To see how well the different kinds of data compresses when used
by zipHMMlib the compression ratio for all the sequences described above have
been measured. The compression ratio is defined as
$\frac{\text{original size}}{\text{compressed size}}$. Hence, the greater
compression ratio the better. Recall, that the amount of compression depends on
the estimate of how many times the algorithm is run after compressing the
sequence. For this experiment an estimate of $e = 500$ have been provided to
the program.

As seen in figure~\ref{fig:compression_ratio} the compression ratio grows
linearly for the Fibonacci words as the sequence length increases, whereas is
it only slightly growing at an approximate ratio of four for random
sequences with alphabet size 4. The compression of Fibonacci words are much
better than the compression of random sequences, just as expected. This
suggests that the performance of the Viterbi algorithm will be very dependent
on the input sequences. Just to verify that the Fibonacci words is very
repetitive, unary sequences have also been compressed, and as seen the
compression of these two types is very comparable.

For this experiment only, random sequences of an alphabet of size 2 was also
tested. It is expected that the random sequences with an alphabet of size 2
will compress two times better than the sequence with an alphabet size 4, since
the most common pair of symbols will occur twice as many times in the sequences
with alphabet size 2 than the sequences with alphabet size 4. This is also seen
in the plot. For random sequences of alphabet size four the compression ratio
is approximately four, while it is almost eight for sequences with alphabet
size 2. This suggests that using the library with e.g.\ protein sequences that
has an alphabet size of 20 may not result in a good performance as good as seen
in the following experiment. To keep the experiments simple and not introduce
overly many plots, an alphabet size of four have been used for the rest of the
experiments. This corresponds to DNA sequences that HMM's are often used with
in the field of bioinformatics.

\begin{figure}
  \centering\ref{compression_legend}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/compression_ratio_one.tex}
    \caption{Estimate $e = 1$.}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/compression_ratio.tex}
    \caption{Estimate $e = 500$.}
  \end{subfigure}
  \caption{The compression ratio of Fibonacci words, random sequences of
    alphabet size 2, random sequences of alphabet size 4, and single character
    sequences.}
  \label{fig:compression_ratio}
\end{figure}

\section{Preprocessing}

As described in section~\ref{sec:saving-compr-sequ} the compression of the
input sequence may be done prior to executing the Viterbi or Forward-Backward
algorithms. This is useful if the algorithms are to be used with different
models, as the compression of the sequence does not depend on the model.

The preprocessing has a theoretical running time of
$O(( \lvert\mathcal{O'}\rvert - \lvert{\mathcal{O}}\rvert) T)$ as stated in
section~\ref{sec:running-time}. In figure~\ref{fig:pre_viterbi_T} the running
time divided by the length of the sequence, $T$, is shown for random sequences
and Fibonacci words of varying length.

Recall that the preprocessing iterates over the observation sequence and finds
the most common pair of symbols, introduces a new symbol, and replaces the pair
with the new symbol.

As seen the running time is not linear in $T$ for random sequences. Since the
compression ratio is dependent on $T$ as also seen in the previous section and
the size of the new alphabet $\lvert\mathcal{O'}\rvert$ grows with the
compression ratio, $\lvert\mathcal{O'}\rvert$ is also dependent on $T$. Hence,
when $T$ grows, $\lvert\mathcal{O'}\rvert$ also grows, and since the length of
the observation sequence only becomes a bit smaller for each iteration of the
preprocessing, what is seen in the figure is then not surprising; the running
time of the preprocessing for random sequences is superlinear in $T$ and not
linear as the theoretical running time suggests.

However, for Fibonacci words the running time seems to be linear in $T$. For
Fibonacci words, the new alphabet size $\mathcal{O'}$ is also dependent on $T$:
in the previous section it was seen that it scales linearly. Due to the
recursive structure of Fibonacci words, the length of the observation sequence
is almost halved each time a new symbol is introduced. Hence, the first
iteration of the preprocessing takes time proportial to $T$, the next to $T/2$,
the next to $T/4$, and so on. So eventhough $\mathcal{O'}$ is dependent on $T$,
the recursive structure makes the running time linear in $T$ since
$T + T/2 + T/4 + T/8 + \cdots < 2T$.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/pre_viterbi_T}
    \caption{Random sequences.}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/pre_viterbi_fib_T}
    \caption{Fibonacci words.}
  \end{subfigure}
  \caption{Preprocessing time divided by observation sequence length for
    varying lengths.}
  \label{fig:pre_viterbi_T}
\end{figure}

\section{Viterbi}

This section covers the experiments made in context of the Viterbi algorithm.
First, the theoretical asymptotic running time of the algorithm is verified to
make sure that the running time of the algorithm behaves as expected. Secondly,
a simple implementation of the classical Viterbi algorithm is compared to the
zipHMMlib implementation.

\subsection{Verification of Theoretical Running Times}
\label{sec:theor-runn-times}

In this section the theoretical running times are verified to make sure that
the implementation runs as expected. The running time has been measured for
three variations of the Viterbi algorithm corresponding to the backtracking
discussed in section~\ref{sec:backtracking}. They are named as follows.
\begin{description}
\item[Viterbi\textsubscript{L}] compresses the sequence and only computes the
  log-likelihood,
\item[Viterbi\textsubscript{P}] also backtracks,
\item[Viterbi\textsubscript{PM}] also saves memory on backtracking.
\end{description}
The next two sections verifies that the implementation follows the theoretical
running times.

\subsubsection{Sequence Length}

The implementation of the Viterbi algorithm supports both computing the Viterbi
path and not doing it. For the two of those there is a difference in the
theoretical time, since computing the path is linear in $T$. For a list of
running times for the algorithms, see table~\ref{tab:running-time}.

In figure~\ref{fig:assymptotic_viterbi_T} the running divided by the length of
the compressed sequence $T'$ is shown for input sequences of varying lengths.
The experiment has been made for both random sequence (to the left) and
Fibonacci words (to the right). As expected the fraction is close to constant
with a slightly decreasing curve for Viterbi\textsubscript{L} in both cases.
For random sequences the running time for Viterbi\textsubscript{P} and
Viterbi\textsubscript{PM} seems to be close to linear in $T'$ also. This is due
to the compression ratio being only approximately four, and that the
backtracking may be done quickly using the $R$ matrices. For Fibonacci word the
compression ratio is much better and the Viterbi\textsubscript{P} and
Viterbi\textsubscript{PM} algorithms are clearly not linear in $T'$.

\begin{figure}
  \centering\ref{test}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/assymptotic_viterbi_T}
    \captionsetup{margin=10pt}
    \caption{For random data the running time is in practice linear in $T'$ as
      the compression ratio is approximately four.}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/assymptotic_viterbi_fib_T}
    \captionsetup{margin=10pt}
    \caption{For the very repetitive Fibonacci words, $T' = 2$. This exploits
      that the backtracking algorithms are far from linear in $T'$.}
  \end{subfigure}
  \caption{The running time of Viterbi algorithms divided by the compressed
    sequence length $T'$. The running time of Viterbi\textsubscript{L} is
    linear in $T'$ whereas Viterbi\textsubscript{P} and
    Viterbi\textsubscript{PM} are superlinear.}
  \label{fig:assymptotic_viterbi_T}
\end{figure}

Both backtracking methods have a theoretical running time linear in the length
of the original sequence $T$. To verify this the running time of the
backtracking itself divided by $T$ for sequences of varying lengths and shown in
figure~\ref{fig:assymptotic_viterbi_backtrack_T}. As seen, it is indeed linear
in $T$.

\begin{figure}
  \centering\ref{test2}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/assymptotic_viterbi_backtrack_T}
    \caption{Random sequences.}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering \input{figures/assymptotic_viterbi_fib_backtrack_T}
    \caption{Fibonacci words.}
  \end{subfigure}
  \caption{The running time of Viterbi as a function of the original sequence
    length $T$. The running time of all three variations of the Viterbi
    algorithm are linear in $T$.}
  \label{fig:assymptotic_viterbi_backtrack_T}
\end{figure}

\subsubsection{Model Size}

The theoretical running times of all three variations of the Viterbi algorithm
are cubic in the number of states $N$. To verify this an experiment with random
sequences of length 10000 have been run for models with between 2 and 1024
states. Fibonacci words has not been used in this experiment since the running
time does not change with the compression. The running time divided by $N^3$ is
shown in figure~\ref{fig:assymptotic_viterbi_backtrack_N}. It is expected that
the points in the plot will form a decreasing curve. \fxwarning{Discuss that
  the plot is ugly as hell. It is probably due to the alignment of the
  matrices. In an ideal world, the matrices should have two repesentations to
  align well in memory. This is however also expensive.}

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_backtrack_N}
  \caption{The running time of Viterbi as a function of the number of hidden
    states in the model $N$. The running time of all three variations of the Viterbi
    algorithm are cubic in $N$.}
  \label{fig:assymptotic_viterbi_backtrack_N}
\end{figure}

Recall that the running time is cubic in $N$ is due to the computation of the
$C$ matrices. The multiplication of the these matrices, i.e.\ the computation
of the Viterbi or forward-backward algorithm, can be done in time
proportional to $N^2$ as stated in section~\ref{sec:running-time}. When the
size of the new alphabet $M'$ is small compared to the length of the compressed
sequence, the $N^3$ term vanishes in practice when compared to the $N^2$
term. Hence, it may be seen that the running time of the matrix multiplication
(and the backtracking) is proportional to $N^2$ by not compressing the
sequence. This is shown in figure~\ref{fig:assymptotic_viterbi_N} where
$\lvert M' \rvert = 4$ and $T = 10.000$.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_N}
  \caption{The running time of Viterbi as a function of the number of hidden
    states in the model $N$. If the input sequences are not compressed, the
    running time of all three variations of the Viterbi algorithm are quadratic
    in $N$.}
  \label{fig:assymptotic_viterbi_N}
\end{figure}

This ends the experiments concerning the theoretical running times of the
Viterbi algorithm itself. It has been shown that the running time in practice
in terms of the length of the observation sequence is very dependent on the
structure of it, but it still fits the theoretical worst case running time. In
terms of the model size the physical hardware becomes visible. However, when
the models get large enough the running time stabalizes and the theoretical
running time is confirmed.

\subsection{Comparing zipHMMlib Viterbi to the Classical Viterbi Algorithm}
\label{sec:comp-ziphmml-viterbi}

First the zipHMMlib Viterbi algorithm is compared to the classical
implementation of the Viterbi algorithm. Recall that the compressed sequence
from the preprocessing may be saved to disk for later use. For some problems
though one might only need to run Viterbi once for a sequence. As described in
section~\ref{sec:compr-stopp-crit} the preprocessing uses an estimate of how
many times the Viterbi algorithm is run to compress the sequence appropriately.
To experiment with this, experiments have been made with an estimate of one and
with 500 executions of Viterbi. In total, six different setups of the Viterbi
algorithm have been measured:
\begin{description}
% \item[Uncompressed] that does not compress the sequence and only find the
%   loglikelihood of the most probable path,
% \item[Uncompressed Path] that also backtracks to find the most likely path,
% \item[Uncompressed Path Memory] that also saves memory in the
%   backtracking process,
\item[Viterbi\textsubscript{L} 1] that compresses the sequence and only computes the
  log-likelihood once,
\item[Viterbi\textsubscript{P} 1] that also backtracks,
\item[Viterbi\textsubscript{PM} 1] that also saves memory on backtracking.
\item[Viterbi\textsubscript{L} 500] that compresses the sequence and computes the
  log-likelihood 500 times,
\item[Viterbi\textsubscript{P} 500] that also backtracks the 500 times,
\item[Viterbi\textsubscript{PM} 500] that also saves memory on backtracking.
\end{description}

The algorithms are compared to their respective counterpart of the classical
algorithm, e.g.\ for Viterbi\textsubscript{L} algorithms the classical
algorithm without backtracking has been used. In the following plots a
``speedup factor'' is shown on the y-axis. The speedup factor is calculated as
the running time of the classical algorithm (with or without without backtracking
enabled) divided by the running time of algorithm being measured, including the
time it takes to do the preprocessing. Hence, any speedup factor larger than one
means that the algorithm being measured is faster than the classical algorithm.

\subsubsection{Random Data}

The first experiments have been made using random sequences.

In figures~\ref{fig:compressed_1_speedup_vs_sequence_length}
and~\ref{fig:compressed_500_speedup_vs_sequence_length} the speedup factor for
the Viterbi 1 algorithms and Viterbi 500 algorithms is shown for sequences of
varying length. A model with 16 states was used for these experiments. As
expected, computing the Viterbi path limits the speedup factor since the
running time is linear in $T$ instead of $T'$ as it is for only computing the
log-likelihood. The Viterbi\textsubscript{P} and Viterbi\textsubscript{PM} gain
a similar speedup. This is expected as the main part of the memory saving in
Viterbi\textsubscript{PM} essentially consists in computing the Viterbi twice.
Since this is done for both the zipHMM implementation and the classical
implementation, the speedup becomes the same as for the
Viterbi\textsubscript{P}. For small sequences the speedup factor is smaller
than for it is for large sequences. This is due to the zipHMM Viterbi
algorithm computes the $C_{o_i}$ matrices before starting the actual Viterbi
computation. As the sequences gets longer this computation becomes a smaller
fraction of the sequence and the speedup factor stabalizes at approximate 32
for this example of the Viterbi\textsubscript{L} algorithm. At sequence length
from $10^5$ to $10^6$ there is a sudden jump in the running time of the
backtracking algorithms. No explanation for this has been found. \fxnote{Then
  find one!} The speedup factors are surprisingly good. Eventhough the
compression ratio is only around four, a speedup of more than a factor of 100
is seen in figure~\ref{fig:compressed_500_speedup_vs_sequence_length}. So even
if the data used with the algorithm does not compress very well large speedup
may still by gained from using matrix multiplications instead of the classical
formulation.

\begin{figure}
  \centering
  \input{figures/compressed_1_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 1 algorithms of the total
    running time for sequences of varying lengths using
    a HMM with 16 states.}
  \label{fig:compressed_1_speedup_vs_sequence_length}
\end{figure}

\begin{figure}
  \fxnote{Why does this not look exactly like figure~\ref{fig:compressed_1_speedup_vs_sequence_length}?}
  \centering
  \input{figures/compressed_500_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 500 algorithms of the total
    running time for sequences of varying lengths using
    a HMM with 16 states.}
  \label{fig:compressed_500_speedup_vs_sequence_length}
\end{figure}

Instead of presenting the speedup factor as a function of the sequence length,
the model size is used in the next experiment. So while the size of the model
is varied the length of the observation sequence is kept constant at length
10000. Recall that the running time of Viterbi is $O(M' N^3 + N^2 T')$. The
cubic term might become a problem as $N$ grows. The speedup factor as function
of model size $N$ is plotted in figures~\ref{fig:speedup_vs_N}
and~\ref{fig:speedup_vs_N2} for the Viterbi 1 and Viterbi 500 algorithms
respectively. As seen the speedup factor is growing in the beginning as the
number of states increases. This is probably due to the matrix representation
of the algorithm making it more efficient. However, for larger matrices, the
cubed $N$ term becomes a limiting factor and the speedup factor drops below 1.
For the Viterbi 500 experiment the peak is seen for a smaller number of states
than for the Viterbi 1 experiment. This is due to the new alphabet size
$\mathcal{O'}$ in being bigger in the Viterbi 500 experiment.

\begin{figure}
  \centering
  \input{figures/speedup_vs_N.tex}
  \caption{The speedup factor for varying model sizes using the Viterbi\textsubscript{L} 1
    algorithms.}
  \label{fig:speedup_vs_N}
\end{figure}

\begin{figure}
  \centering
  \input{figures/speedup_vs_N2.tex}
  \caption{The speedup factor for varying model sizes using the Viterbi\textsubscript{L} 500
    algorithms.}
  \label{fig:speedup_vs_N2}
\end{figure}

\subsubsection{Fibonacci Words}

Even though a good speedup were gained for many cases using random data, it is
expected that much greater speedups may be obtained by using repetitive
sequences. Thus, experiments similar to the ones for random data has been
conducted. In figures~\ref{fig:fib_compressed_1_speedup_vs_sequence_length}
and~\ref{fig:fib_compressed_500_speedup_vs_sequence_length} the speedup factor
is shown for sequences of varying length using 1 or 500 runs respectively.

\begin{figure}
  \centering
  \input{figures/fib_compressed_1_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 1 algorithms of the total
    running time for Fibonacci words of varying
    lengths using a HMM with 16 states.}
  \label{fig:fib_compressed_1_speedup_vs_sequence_length}
\end{figure}

\begin{figure}
  \centering
  \input{figures/fib_compressed_500_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 500 algorithms of the total
    running time for Fibonacci words of varying
    lengths using a HMM with 16 states.}
  \label{fig:fib_compressed_500_speedup_vs_sequence_length}
\end{figure}

As seen, the speedup factors are much greater than for random data. For random
data the computation of the Viterbi path limited the speedup factor. This
limitation becomes larger for Fibonacci words. This may e.g.\ be seen in
figure~\ref{fig:fib_compressed_500_speedup_vs_sequence_length}, where the
computation of the Viterbi path makes the computation orders of magnitudes
slower than just computing the likelihood. However, the speedup factor for the
Viterbi\textsubscript{P} and Viterbi\textsubscript{PM} algorithms still perform
better on Fibonacci words than on random sequences.

\subsubsection{Future Work}

To get an impression of where the bottleneck of the algorithm is, the running
time of the preprocessing and the execution of the Viterbi algorithm has been
measured. To keep the number of graphs at a reasonable level, the experiment
has only been made using random data and only the Viterbi\textsubscript{L}
algorithm have been used. Similar results may be obtained using the
Viterbi\textsubscript{P} and Viterbi\textsubscript{PM} algorithms. Again, for
the Viterbi 1 the preprocessing has been done and then Viterbi has been
executed once. For Viterbi 500 the preprocessing has been done followed by 500
executions of Viterbi. The result can be seen in
figure~\ref{fig:pre_vs_running}.

\begin{figure}
  \centering\ref{named}
  \begin{subfigure}{0.5\textwidth}
    \centering \input{figures/viterbi_pre_vs_running_one.tex}
    \caption{Viterbi\textsubscript{L} 1}
  \end{subfigure}%
  \begin{subfigure}{0.5\textwidth}
    \centering \input{figures/viterbi_pre_vs_running_many.tex}
    \caption{Viterbi\textsubscript{L} 500}
  \end{subfigure}

  % \begin{subfigure}{0.5\textwidth}
  %   \centering \input{figures/viterbi_pre_vs_running_one_path_memory.tex}
  %   \caption{Viterbi\textsubscript{PM} 1}
  % \end{subfigure}%
  % \begin{subfigure}{0.5\textwidth}
  %   \centering \input{figures/viterbi_pre_vs_running_many_path_memory.tex}
  %   \caption{Viterbi\textsubscript{PM} 500}
  % \end{subfigure}
  \caption{Fraction of the running time spent on preprocessing and Viterbi
    algorithm for input sequences of varying length. For the Viterbi 1
    experiment the largest part of the running time comes from the
    preprocessing, whereas the fraction of time spent on preprocessing for
    the Viterbi 500 experiment is almost zero.}
  \label{fig:pre_vs_running}
\end{figure}

For the Viterbi\textsubscript{L} 1 experiment it is seen that a large part of the running time
is spent on the preprocessing. However, as the number of executions is
increased the time of the preprocessing vanishes. In terms of performance
gains, making the preprocessing more efficient will only give a gain in speedup
when the number of executions of the Viterbi algorithm is low, while higher
efficiency in the Viterbi algorithm or in the effectiveness of the compression
will make the running time smaller for many executions.

\subsubsection{Conclusion}

This ends the section experimenting with the Viterbi algorithm. It is concluded
that the compression of sequences does indeed speed up the execution time of
the Viterbi algorithm. The best results are of course found for repetitive data
with the Viterbi algorithm run multiple times. Speedup factors in the order of
thousands were found, so for these kinds of problems this method has a great
potential. In other scenarios with random data a more modest speedup was
obtained, depending on the size of the model.

\section{Posterior Decoding}

This section contains all experiments performed on the posterior decoding
algorithm. As described in section~\ref{sec:probl-expl-repet}, no efficient way
of exploiting sequence repetitions has been found. Nevertheless, experiments
have been made to see whether a minor speedup can be obtained by computing the
posterior decoding in the matrix multiplications framework given by
zipHMMlib. In contrast to the previous section with multiple variations of the
Viterbi algorithm, only a single variation makes sense here: the input sequence
is not compressed and the path is computed.

The section is split into two parts. The first part compares the actual
running time to the theoretical running time to verify that the implementation
of the algorithm satisfies the theory. The second part compares it to the
classical implementation of posterior decoding.

\subsection{Asymptotic Running Time}
\label{sec:asympt-runn-time}

Like in section~\ref{sec:theor-runn-times}, the theoretical compared the
actual running times experiments have been made using random data. The
theoretical running time is $O(M N^3 + TN^2)$. Since $M$ is very small compared
to $T$, is expected that the first term vanishes. This was already seen for the
Viterbi algorithm in section~\ref{sec:theor-runn-times}. Hence, for this
experiment the theoretical running time $O(TN^2)$ is assumed. This of course
only makes sense if $N$ is not too large, but as seen in the plots the
assumption of a theoretical running time of $O(TN^2)$ does make sense for these
experiments.

First, the running time compared to the sequence length is shown in
figure~\ref{fig:posterior_T}. A model with 16 states has been used. As expected,
the running time is decreasing going towards constant.

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_T.tex}
  \caption{The running time of posterior decoding is linear in the size of the
    input sequence.}
  \label{fig:posterior_T}
\end{figure}

Secondly, in figure~\ref{fig:posterior_N} the running time for an increasing
number of states is shown. A random sequence of length $10.000$ has been used. It
can be seen that the running time is indeed quadratic in the numbers of states
in practice.

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_N.tex}
  \caption{The running time of posterior decoding is quadratic in the number of
    states.}
  \label{fig:posterior_N}
\end{figure}

It is concluded that the actual running time of the algorithm follows the
theoretical.

\subsection{Comparison to the Classical Algorithm}
\label{sec:comp-class-algor}

Now the algorithm is compared the classical algorithm. In
figure~\ref{fig:posterior_speedup_vs_sequence_length} the speedup factor as
function of the sequence length is shown. Random sequences and a model with 16
states was used for the experiment. In all tested cases the algorithm is a bit
faster than the classical implementation. There is a tendency that the speedup
slowly becomes larger as the input sequence grows. Speedup factors between 1
and 5 are gained in this experiment. This may change if the number of states in
the model is different.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for sequences of varying lengths using a HMM
    with 16 states.}
  \label{fig:posterior_speedup_vs_sequence_length}
\end{figure}

Looking at the speedup factor as function of the number of states in
figure~\ref{fig:posterior_speedup_vs_N}, it is seen that for small models it is
below 1. This means that the algorithm is slower than the classical posterior
decoding algorithm. However, as the number of states in the model increases the
speedup factor quickly becomes larger than 1, due to the matrix multiplications
being very efficient due to the BLAS framework.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_N.tex}
  \caption{The speed up factor as function of the model size using a sequence
    of length 10000.}
  \label{fig:posterior_speedup_vs_N}
\end{figure}

These experiments end the section of posterior decoding. Overall the algorithm
has a running time that is very comparable to the classical implementation of
posterior decoding. For scenarios with large models and sequences the zipHMMlib
is faster whereas the classical algorithm is a better for small models.

\section{Indexed Posterior Decoding}

In this section experiments on the indexed posterior decoding is
made. Recall that indexed posterior decoding also takes two indices $i$ and
$j$ as input and only returns the posterior decoding for the subsequence
$Y_{i:j}$.

Recall from section~\ref{sec:running-time-2} that the algorithm has a
theoretical running time of $O(M' N^3 + N^2 T' + N^2 (j - i + \log T))$,
assuming the input sequence has already been compressed. This is very similar
to both the Viterbi algorithm and the posterior decoding algorithm. Therefore,
the verification of the theoretical running in practice has been omitted
However, the three terms of the running time are still discussed in turn below.

The first term $M' N^3$ comes from the computation of the $C$ matrices. This
has already been experimented with for the Viterbi algorithm in
section~\ref{sec:theor-runn-times}. Since the only difference between the
computations for the Viterbi algorithm and the forward-backward algorithm has
to do with numerical stability, the theoretical running will be also hold for
the indexed posterior decoding.

The second term $N^2 T'$ comes from computing the forward and backward tables
for the compressed sequence of length $T'$. In
section~\ref{sec:asympt-runn-time} this was already verified the for posterior
decoding algorithm.

Finally, the $N^2 (j - i + \log T)$ term comes from the decompression of the
compressed subsequence that is linear in $j - i + \log T$ and from computing
the posterior decoding for the subsequence. The running time of the posterior
decoding has already been shown to match the theoretical running time. The
decompression is quite simple and experiments have not been done for
this. \fxwarning{Make experiment for decompression?}

\subsection{Comparison to the Classical Algorithm}

The indexed posterior decoding algorithm has been compared to the classical
posterior decoding algorithm. For the classical algorithm the entire path as
been found and then the path from index $i$ to $j$ has been extracted.

As for the discussion of the Viterbi algorithm, the comparison of to the
classical implementation is first made for random data and then for Fibonacci
words. Like for the Viterbi algorithm the algorithm is analyzed by varying the
input sequence length and the model size, but the length of the subsequence $Z$
also has an impact on the running time, so this has also been experimented
with.

\subsection{Random Data}

For the first experiment the distance between the two indices $i$ and $j$ has
been varied for random input sequences of length $10,000$ and a model with 16
states. For this experiment the ``extra'' symbols corresponding to the indices
$[k, i)$ and $(j, l]$ can be left out as there is at most $\log 10,000$ of
these, which is a small number compared to $j - i$.

The running time of the indexed posterior decoding is much dependent on the
size of the size of the interval for which to compute the path, i.e.\ $j-i$.

Depending on how well the data compresses, the value of the length of the
compressed sequence $T'$ will be larger or smaller than the length of
subsequence $Z$.

 For random data that only has a compression ratio of
approximately 4 as seen section~\ref{sec:compression-ratio} the number of
``extra'' symbols will be small. Hence, when $j - i$ is small the length of $Z$
will also be small compared to $T'$. Therefore, it is expected that $T'$ will
dominate the running time and small increases of $j - i$ will not make major
changes to the overall running time. However, for large values of $j - i$, the
value will be close to or larger than $T'$ and the value will have a larger
impact on the running time, thus making the running time higher.

This is seen in
figure~\ref{fig:assymptotic_indexed_posterior_subseq_length.tex} where the
running time is nearly constant for small values of $j - i$ but starts to grow
when $j - i$ gets closer to $T'$.

\begin{figure}
  \centering
  \input{figures/assymptotic_indexed_posterior_subseq_length.tex}
  \caption{The running time of indexed posterior decoding for varying distances
    between $i$ and $j$. Random sequences has been used.}
  \label{fig:assymptotic_indexed_posterior_subseq_length.tex}
  \fxnote{Mention that the running time increases with almost a factor three
    due to the computation of almost the entire observation sequence that is
    much slower than doing it on the compressed sequence.}
\end{figure}

In figure~\ref{fig:indexed_posterior_speedup_vs_subseq} the running time is
compared the classical algorithm. A speedup is gained when running the
algorithm multiple times, but for a single run the algorithm is slower than the
classical one in this case. The speedup get smaller when $j - i$ gets closer to
$T$. That is expected from the running time seen in
figure~\ref{fig:assymptotic_indexed_posterior_subseq_length.tex} that was
discussed in the previous paragraph.

\begin{figure}
  \centering
  \input{figures/indexed_posterior_speedup_vs_subseq.tex}
  \caption{The indexed posterior decoding algorithm compared to the simple
    algorithm for random data for varying distances between $i$ and $j$.}
  \label{fig:indexed_posterior_speedup_vs_subseq}
\end{figure}

For the second experiment the length of the input sequence has been varied from
small sequences of length $10^3$ to approximately $10^7$. As already seen in
sections~\ref{sec:comp-ziphmml-viterbi} and~\ref{sec:comp-class-algor} the
longer the input sequence, the better speedup for the Viterbi algorithm and the
uncompressed posterior decoding. It is expected that the indexed posterior
decoding will be similar to the uncompressed posterior decoding algorithm. The
result of this experiment is shown in
figure~\ref{fig:indexed_posterior_speedup_vs_T}. The result is as
expected. Again, the indexed posterior decoding 1 algorithm is slower than
the classical algorithm for smaller sequences. For the indexed posterior
decoding 500 algorithm however, speedups of up to factor 15 is seen in this
example.

\begin{figure}
  \centering
  \input{figures/indexed_posterior_speedup_vs_T.tex}
  \caption{The indexed posterior decoding algorithm compared to the simple
    algorithm for random data for varying input sequence lengths.}
  \label{fig:indexed_posterior_speedup_vs_T}
\end{figure}

Finally, the number of states in the model has been varied. Sequences of length
10000 were used and the posterior decoding has been computed for an indexed of
length 200. As for the Viterbi algorithm experiments in
section~\ref{sec:comp-ziphmml-viterbi} it is expected that the algorithm is
most efficient for ``medium sized'' models. As seen in
figure~\ref{fig:indexed_posterior_speedup_vs_N} that is also the case. The
result is very similar to the results seen in figures~\ref{fig:speedup_vs_N}
and~\ref{fig:speedup_vs_N2}.

\begin{figure}
  \centering
  \input{figures/indexed_posterior_speedup_vs_N.tex}
  \caption{The indexed posterior decoding algorithm compared to the simple
    algorithm for random data for varying model sizes.}
  \label{fig:indexed_posterior_speedup_vs_N}
\end{figure}

\subsubsection{Fibonacci Words}

The results of the experiments using random data showed potential for speedups
in some cases whereas in other cases slowdowns were obtained. As seen for the
Viterbi algorithm much better results were obtained by using Fibonacci words,
due to their repetitive nature.

In this section the experiments from before is repeated with Fibonacci words
instead of random sequences. It is expected that this will provide better
speedups.

For the first experiment with a fixed sequence length and model size, the
situation is a bit more complicated. Recall again from
section~\ref{sec:compr-stopp-crit} that the user specifies a parameter $e$ that
is the expected number of executions of the algorithm following the
compression. Since Fibonacci words compresses very well, the compression is very
dependent on this parameter. As seen in section~\ref{sec:compression-ratio},
the compression ratio varies a lot. \fxerror{Make experiment!} As the number of
``extra'' symbols depends on the compression ratio, $Z$ will become larger when
$e$ becomes larger. Therefore, if $e$ is large, the value of $j - i$ will not
change the running time as $Z$ is large due to the ``extra'' symbols anyways.
In the case of Fibonacci words and $e = 500$, $T'$ will be only 2. Hence, it is
expected that $Z$ will be large even when $j - i$ is small, and hence the
running time will not change for small changes to $j - i$. The result of this
experiment is shown in
figure~\ref{fig:assymptotic_indexed_posterior_fib_subseq_length.tex}. As seen
the result is not as expected as the length of the subsequence indeed has an
impact on the running time. The plot is very similar to the case of random
data. \fxerror{Explain this.}

\begin{figure}
  \centering
  \input{figures/assymptotic_indexed_posterior_fib_subseq_length.tex}
  \caption{The running time for indexed posterior decoding for
    varying indexed lengths. Fibonacci word has been used}
  \label{fig:assymptotic_indexed_posterior_fib_subseq_length.tex}
\end{figure}

Since the plot of the running time is very similar to the one for random data,
the plot of the speedup when compared the classical algorithm is also very
similar. This is seen in
figure~\ref{fig:fib_indexed_posterior_speedup_vs_subseq}. The dots form a
curve similar to the curve for random data, and as expected the speedup is
better than for random data.

\begin{figure}
  \centering
  \input{figures/fib_indexed_posterior_speedup_vs_subseq.tex}
  \caption{The indexed posterior decoding algorithm compared to the simple
    algorithm for a Fibonacci word for varying substring lengths.}
  \label{fig:fib_indexed_posterior_speedup_vs_subseq}
\end{figure}

As before the speedup factor has also been measured for varying sequence
lengths. Again, a model with 16 states has been used and the posterior decoding
has been computed for a substring of length 200. The result of this experiment
is shown in figure~\ref{fig:indexed_posterior_fib_speedup_vs_T}. As seen, the
speedup factor improves by a lot for the indexed posterior 500 algorithm. For
the indexed posterior 1 algorithm the speedup factor is more modest. However,
the longer the input sequence, the better the speedup.

\begin{figure}
  \centering
  \input{figures/indexed_posterior_fib_speedup_vs_T.tex}
  \caption{The indexed posterior decoding algorithm compared to the simple
    algorithm for Fibonacci words of varying lengths.}
  \label{fig:indexed_posterior_fib_speedup_vs_T}
\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-command-extra-options: "-shell-escape"
%%% TeX-master: "master"
%%% End:
