% chktex-file 24
% chktex-file 44
\chapter{Experiments}

This chapter covers description and discussion of all experiments performed in
this thesis. First, the experimental setup is described, followed by the data
used, and then the Viterbi and posterior algorithms are discussed in turn.

The experiments have been run on a PC with an Intel Xeon W3550 $3.07$ GHz CPU
and 4 GB RAM running GNU/Linux 3.13. Each point in the plots in this chapter is
formed by the mean of multiple runs. In the case of random sequences, 20 random
sequences of the same length has been generated and the mean of the measure
calculated. In case e.g.\ Fibonacci words, only a single sequence of a
specified length exist, so in this case the experiment has been performed five
times and the mean calculated to even out fluctuations in the
measure. Furthermore, the standard deviation has been calculated and is shown
as error bars in the plots.

\section{Data}

Hidden Markov models have successfully been applied to various kinds of
data. \fxwarning{Reference} As this thesis is written with no specific kind of
data in mind, aside from repetitive data of course, the experiments primarily
use random data that does not contain many repetitions and Fibonacci words that
is a highly repetitive kind of sequence. \fxnote{DNA data?}

The models used is random, fully connected models, that is there is a
transition from a state to any other state. The emission probabilities are
random too. 10 random models with $2, 4, 8, \dots, 1024$ states have been
generated.

The input sequences used is now described in turn.

\begin{description}
\item[Random sequences] are generated by one of the random models by making
  transitions and emissions according to the parameters of that model. The
  generated sequences have length $10^3$ to approximately $10^7$.
\item[Fibonacci words] are generated in a way very similar to how Fibonacci
  numbers are generated by concatenating the two previous words instead of
  adding them. Let $S_0$ be ``0'' and $S_1$ be ``01''. Now
  $S_n=S_{n-1}S_{n-2}$. In contrast to the random sequences, Fibonacci words
  contain many repetitions and compress very well using byte-pair
  encoding. \fxwarning{Reference.}
\item[DNA sequences] \fxerror{Write something or delete.}
\end{description}

\section{Compression Ratio}
\label{sec:compression-ratio}

The running time of the Viterbi algorithm implemented in zipHMMlib is highly
dependent on the size of the compressed sequence. As mentioned in
section~\ref{sec:compr-stopp-crit} it is possible to compress the sequence to a
single character, even though this is not a good idea in terms of
performance. To see how well the different kinds of data compresses when used
by zipHMMlib the compression ratio for all the sequences described above have
been measured. The compression ratio is defined as
$\frac{\text{original size}}{\text{compressed size}}$. Hence, the greater
compression ratio the better.

As seen in figure~\ref{fig:compression_ratio} the compression ratio grows
exponentially as the sequence length increases. The compression of Fibonacci
words are much better than the compression of random sequences, just as
expected. The Fibonacci words compresses orders of magnitudes better than
random data. This already suggests that the performance of the Viterbi
algorithm will be very dependent on the input sequences.  \fxwarning{Discuss
  random sequences of alphabet size 2, DNA, and single character sequences?}

\begin{figure}
  \centering
  \input{figures/compression_ratio.tex}
  \caption{The compression ratio of Fibonacci words, random sequences of
    alphabet size 2, random sequences of alphabet size 4, DNA
    sequences, and single character sequences.}
  \label{fig:compression_ratio}
\end{figure}

\section{Viterbi}

This section covers the experiments made in context of the Viterbi
algorithm. First, the theoretical asymptotic running time of the algorithm is
verified to make sure that the running time of the algorithm behaves as
expected. Secondly an implementation of the classical Viterbi algorithm is
compared to the zipHMMlib implementation with compressed both enabled and
disabled.

\subsection{Verification of Theoretical Running Times}
\label{sec:theor-runn-times}

In this section the theoretical running times are verified to make sure that
the implementation runs as expected. The running time has been measured
seperately for the preprocessing and Viterbi with and without backtracking
enabled.

\subsubsection{Preprocessing}

As described in section~\ref{sec:saving-comp-stuff} the compression of the
input sequence may be done prior to executing the Viterbi algorithm. This is
useful if the Viterbi algorithm is run with different models, as the
compression of the sequence does not depend on the model.

The first experiment in this section is verifying the theoretical running time
of the preprocessing that has a theoretical running time of
$O( \left( \lvert\mathcal{O'}\rvert - \lvert{\mathcal{O}}\rvert \right) T)$ as
stated in section~\ref{sec:running-time}.

In figure~\ref{fig:pre_viterbi_n} the running time divided by the length of the
sequence is shown for random sequences of varying length. \fxnote{Should I make
a better experiment also showing the size of the new alphabet?}

\begin{figure}
  \centering
  \input{figures/pre_viterbi_n}
  \caption{zipHMMlib preprocessing time for varying sequence lengths.}
  \label{fig:pre_viterbi_n}
\end{figure}

% \begin{figure}
%   \centering
%   \input{figures/pre_viterbi_k}
%   \caption{zipHMMlib preprocessing time for varying model sizes.}
%   \label{fig:pre_viterbi_k}
% \end{figure}

\subsubsection{Running Time Without Backtracking}

The implementation of the Viterbi algorithm supports both computing the Viterbi
path and not doing it. For the two of those there is a difference in the
theoretical time, since computing the path takes $O(T)$ time.

As stated in section~\ref{sec:running-time} the running time without
backtracking is linear in the length of the compressed sequence and cubic in
the number of states.

In figure~\ref{fig:assymptotic_viterbi_n} the running divided by the length of
the compressed sequence is shown for input sequences of varying lengths. As
expected this fraction is close to constant with a slightly decreasing curve,
thus verifying that the algorithm is linear in $T'$.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_n}
  \caption{zipHMMlib running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_n}
\fxnote{Divide the running time by the length of the compressed sequence.}
\end{figure}

A similar experiment has been conducted to verify that the running time i cubic
in the number of states. This is shown in
figure~\ref{fig:assymptotic_viterbi_k}. Again, as expected the fraction is
decreasing and goes towards being constant.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_k}
  \caption{zipHMMlib running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_k}
\fxnote{Make experiment without compression enabled? Then the running time
  should be only quadratic in the number of states.}
\end{figure}

\subsubsection{Running Time With Backtracking}

In most cases the Viterbi path is needed. Thus, backtracking has to be
done. This takes time linear in the length $T$ of the original sequence. To verify
this the running time of the backtracking itself divided by $T$ for sequences
of varying lengths and show in
figure~\ref{fig:assymptotic_viterbi_backtrack_n}. As seen it is indeed
linear. To make sure that the backtracking is constant in the number of states,
the running time is shown for various number of states in
figure~\ref{fig:assymptotic_viterbi_backtrack_k}. \fxwarning{What do we see?}

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_backtrack_n}
  \caption{zipHMMlib Path backtracking time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_backtrack_n}
\end{figure}

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_backtrack_k}
  \caption{zipHMMlib Path backtracking time for varying model sizes.}
  \label{fig:assymptotic_viterbi_backtrack_k}
\end{figure}

For completeness the total running time of the Viterbi algorithm including
backtracking is shown in figures~\ref{fig:assymptotic_viterbi_path_n} and
\ref{fig:assymptotic_viterbi_path_k}. The running time is divided by $T$ and
$N^3$ respectively, and as seen this still forms a decreasing curve going
towards a constant.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_path_n}
  \caption{zipHMMlib Path running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_path_n}
\end{figure}

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_path_k}
  \caption{zipHMMlib Path running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_path_k}
\end{figure}

This ends the experiments concerning the theoretical running times of the
Viterbi algorithm itself.

\subsection{Comparing zipHMMlib to the Classical Viterbi Algorithm.}

First the zipHMMlib Viterbi algorithm is compared to the classical
implementation of the Viterbi algorithm. Recall that the preprocessing may be
saved to disk for later use. For some problems one might only need to run
Viterbi once for a sequence. In this case it is relevant to look at the running time
including preprocessing. Hence, the experiments that both include and exclude
the preprocessing time have been made.

Four variations of the Viterbi algorithm have been measured:
\begin{description}
\item[Uncompressed] that does not compress the sequence and only find the log
  likelihood of the most probable path,
\item[Uncompressed Path] that also backtracks to find the most likely path,
\item[Compressed] that compresses the sequence and only computes the
  likelihood,
\item[Compressed Path] that also backtracks.
\end{description}

In the following plots a ``speedup factor'' is shown on the y-axis. The speedup
factor is calculated as the running time of the classical algorithm (with out
without backtracking enabled) divided by the running time of algorithm being
measured. Hence, any speedup factor larger than one means that the algorithm
being measured is faster than the classical algorithm.

% \begin{figure}
%   \centering
%   \input{figures/speedup_vs_complexity.tex}
%   \caption{Running time vs.\ sequence complexity using sequences of length $10^6$.}
%   \label{fig:speedup_vs_complexity}
% \end{figure}

\subsubsection{Random Data}

The first experiments have been made using random sequences. First the speedup
including preprocessing is shown. In this case it might be better to not
compress the sequence, since a lot of time is spent doing the
preprocessing.

In figure~\ref{fig:speedup_vs_sequence_length} the speedup factor for these
four algorithms is shown for sequences of varying length. Interestingly the
speedup of the Uncompressed algorithm is increasing as the input length of the
input sequence increases. This is not expected since the Simple algorithm and
the Uncompressed algorithm have the same theoretical running times. For the
Compressed algorithm it is seen that as the length of the input sequence
becomes very large the speedup tends to decrease due to spending more time on
the compression. \fxnote{Should this be avoided by the compression stopping
  criterion?} Comparing Compressed and Uncompressed it is seen that the latter
is superior.

As expected, computing the path limits the speedup increasingly as the length
of the input sequence becomes larger. \fxwarning{Investigate why there is such
  a huge difference between Uncompressed and Uncompressed Path.}
\fxwarning{Elaborate.}

\begin{figure}
  \centering
  \input{figures/speedup_vs_sequence_length.tex}
  \caption{The speed up factor of the total running time including preprocessing
    for sequences of varying lengths using a HMM with 16 states.}
  \label{fig:speedup_vs_sequence_length}
\end{figure}

The next experiment still excludes the preprocessing. This will give the
Compressed algorithm a boost, since a lot of time is spent compressing the
sequence, especially for large input sequences. The result of this in shown in
figure~\ref{fig:speedup_vs_sequence_length2}. As seen the Compressed algorithms
now gain a larger speedup than the Uncompressed ones. It is though noticeable
that the Viterbi path retrieval is very expensive yielding a much lower speedup
than just computing the log likelihood.

\begin{figure}
  \centering
  \input{figures/speedup_vs_sequence_length2.tex}
  \caption{The speed up factor of the running time excluding preprocessing for
    sequences of varying lengths using a HMM with 16 states.}
  \label{fig:speedup_vs_sequence_length2}
\end{figure}

Finally, instead of viewing the speedup factor as a function of the sequence
length, the model size is used. Recall that the running time of Viterbi is
$O(M' N^3 + N^2 T')$. Hence, the cubic factor might become a problem as $N$
grows. The speedup factor as function of model size $N$ is plotted in
figures~\ref{fig:speedup_vs_k} and~\ref{fig:speedup_vs_k2} including and
excluding the preprocessing. As expected the speedup continues growing until a
certain number of states and the starts to become smaller again. It is
noticeable though that the good speedup factors are gained for quite large
models of hundreds of states.

\begin{figure}
  \centering
  \input{figures/speedup_vs_k.tex}
  \caption{Running time vs.\ model size including preprocessing time.}
  \label{fig:speedup_vs_k}
\end{figure}

\begin{figure}
  \centering
  \input{figures/speedup_vs_k2.tex}
  \caption{Running time vs.\ model size excluding preprocessing time.}
  \label{fig:speedup_vs_k2}
\end{figure}

In conclusion the zipHMMlib implementation of Viterbi obtains a speedup on
random data given the right circumstances. In all cases a much larger speedup
is obtained if the Viterbi path is not needed. If one only needs to run the
algorithm once, compression should be either enabled or disabled depending on
whether the path is requested or not. If the Viterbi algorithm is run for
multiple models, a speedup is gained by compressing the sequence. If the model
becomes too large though the algorithm fails at gaining a speedup.

\subsubsection{Fibonacci Words}

Even though good speedup factors were gained for random data, it is expected
that much greater speedups may be obtained by using repetitive
sequences. Thus, experiments similar to the ones for random data has been
conducted. Again, a plot including the preprocessing time is shown and then a
plot excluding preprocessing is shown in
figures~\ref{fig:fib_speedup_vs_sequence_length} and
\ref{fig:fib_speedup_vs_sequence_length2} respectively.

Here the largest speedup is in both cases gained by using the Compression
algorithm. For the experiment including the preprocessing speedups of up to a
factor 120 is gained, while the speedups of the experiment excluding
preprocessing are growing exponentially as the sequence length
increases. This is an effect of the good compression ratio of Fibonacci words
discussed in section~\ref{sec:compression-ratio}.

\begin{figure}
  \centering
  \input{figures/fib_speedup_vs_sequence_length.tex}
  \caption{The speed up factor of the total running time include preprocessing
    for Fibonacci words of varying lengths using a HMM with 16 states.}
  \label{fig:fib_speedup_vs_sequence_length}
\end{figure}

\begin{figure}
  \centering
  \input{figures/fib_speedup_vs_sequence_length2.tex}
  \caption{The speed up factor of the running time excluding preprocessing for
    Fibonacci words of varying lengths using a HMM with 16 states.}
  \label{fig:fib_speedup_vs_sequence_length2}
\end{figure}

This ends the section experimenting with the Viterbi algorithm. It is concluded
that the compression of sequences does indeed speed up the execution time of
the Viterbi algorithm. But to gain a speed up, some restrictions are
required. If the number of states in the model becomes too large, a speed up is
gained. Secondly, a speedup is not necessarily gained if the Viterbi algorithm
is only run a single time for a sequence, since the preprocessing/compression of
the sequence takes too much time relatively to the execution of the Viterbi
algorithm. This highly depends on the sequence though. For the very repetitive
Fibonacci words, a speedup was gained even by running the Viterbi algorithm once.

\section{Posterior Decoding}

This section contains all experiments performed on the posterior decoding
algorithm. As described in section~\ref{sec:probl-expl-repet}, no efficient way
of exploiting sequence repetitions has been found. Nevertheless experiments
have been made to see whether a minor speedup can be obtained by computing the
posterior decoding in the matrix multiplications framework. Hence, only one of
the four variations introduced in the Viterbi experiments makes sense here,
namely the Uncompressed Path variation, as the path is always wanted when doing
posterior decoding.

The section is split into two parts. The first part compares the actual
running time to the theoretical running time to verify that the implementation
of the algorithm satisfies the theory. The second part compares it to the
classical implementation of posterior decoding.

\subsection{Asymptotic Running Time}

Like in section~\ref{sec:theor-runn-times}, the theoretical compared the the
actual running times experiments have been made using random data. The
theoretical running time is $O(M N^3 + TN^2)$. Since $M$ is very small compared
to $T$ is expected that the first term vanishes. Hence, for this experiment the
theoretical running time $O(TN^2)$ is assumed. This of course only makes sense
if $N$ is not too large, but as seen in the plots the assumption of a
theoretical running time of $O(TN^2)$ does make sense for these experiments.

First, the running time compared to the sequence length is shown in
figure~\ref{fig:posterior_n}. A model with 16 states has been used. As expected
the running time is slightly decreasing going towards constant. \fxerror{Remove
  that stupid outlier in the end.}

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_n.tex}
  \caption{The running time of posterior decoding is linear in the size of the
    input sequence.}
  \label{fig:posterior_n}
\end{figure}

Secondly, in figure~\ref{fig:posterior_k} the running time for an increasing
number of states is shown. A random sequence of length 10000 has been used. It
can be seen that the running time is indeed quadratic in the numbers of states.

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_k.tex}
  \caption{The running time of posterior decoding is quadratic in the number of
    states.}
  \label{fig:posterior_k}
\end{figure}

It is concluded that the actual running time of the algorithm follows the
theoretical.

\subsection{Comparison to the Classical Algorithm}

Now the algorithm is compared the classical algorithm. In
figure~\ref{fig:posterior_speedup_vs_sequence_length} the speedup factor as
function of the sequence length is shown. As seen for small sequences the
zipHMMlib algorithm is slower than the classical implementation, whereas for
larger sequences a minor speedup is gained. However, the speedup for sequences
at any length in the experiment is between $0.5$ and 2. Hence, it almost does
not matter  which of the algorithms is used.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for sequences of varying lengths using a HMM
    with 16 states.}
  \label{fig:posterior_speedup_vs_sequence_length}
\end{figure}

Looking at the speedup factor as function of the number of states in
figure~\ref{fig:posterior_speedup_vs_k}, the results are similar to the ones
from the Viterbi algorithm. At some point the speedup factor becomes smaller
due to the $M N^3$ factor. Again it is seen that the speedup factor is within a
small interval of $(0.5, 4)$.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_k.tex}
  \caption{The speed up factor as function of the model size using a sequence
    of length 10000.}
  \label{fig:posterior_speedup_vs_k}
\end{figure}

These experiments ends the section of posterior decoding. Overall the algorithm
has a running time that is very comparable to the classical implementation of
posterior decoding.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
