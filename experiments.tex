% chktex-file 24
% chktex-file 44
\chapter{Experiments}

This chapter covers description and discussion of all experiments performed in
this thesis. First, the experimental setup is described, followed by the data
used, and then the Viterbi and posterior algorithms are discussed in turn.

\section{Data}

Hidden Markov models have successfully been applied to various kinds of
data. \fxwarning{Reference} As this thesis is written with no specific kind of
data in mind, aside from repetitive data of course, the experiments primarily
use random data that does not contain many repetitions and Fibonacci words that
is a highly repetitive kind of sequence. The two types of sequences used is now
described in turn.

\begin{description}
\item[Random sequences] are generated by one of the models (described below) by
  making random transitions and emissions according to the probabilities
  specified by the parameters of that model. The generated sequences varies in
  length from $10^3$ to approximately $10^7$.
\item[Fibonacci words] are generated in a way very similar to how Fibonacci
  numbers are generated by concatenating the two previous words instead of
  adding them. Let $S_0$ be ``0'' and $S_1$ be ``01''. Now
  $S_n=S_{n-1}S_{n-2}$. In contrast to the random sequences, Fibonacci words
  contain many repetitions and compress very well using byte-pair
  encoding. \fxwarning{Reference.}
\end{description}

The models used are random, fully connected models, that is there is a
transition from a state to any other state. The emission probabilities are
random too. The alphabet size has been kept constant at four. In general, the
larger the alphabet, the wrose compression ratio is expected. The size of the
models, i.e.\ the number of states have been varied from 2 to 512.

\section{Experimental Setup}

The experiments have been run on a PC with an Intel Xeon W3550 $3.07$ GHz CPU
and 4 GB RAM running GNU/Linux 3.13. Each point in the plots in this chapter is
formed by the mean of multiple runs. In the case of random sequences, 20 random
sequences of the same length has been generated and the mean of the measure
calculated. In case e.g.\ Fibonacci words, only a single sequence of a
specified length exist, so in this case the experiment has been performed five
times and the mean calculated to even out fluctuations in the
measure. Furthermore, the standard deviation has been calculated and is shown
as error bars in the plots.

\section{Compression Ratio}
\label{sec:compression-ratio}

The running time of the Viterbi algorithm implemented in zipHMMlib is highly
dependent on the size of the compressed sequence. As mentioned in
section~\ref{sec:compr-stopp-crit} it is possible to compress the sequence to a
single character, even though this is not a good idea in terms of
performance. To see how well the different kinds of data compresses when used
by zipHMMlib the compression ratio for all the sequences described above have
been measured. The compression ratio is defined as
$\frac{\text{original size}}{\text{compressed size}}$. Hence, the greater
compression ratio the better.

As seen in figure~\ref{fig:compression_ratio} the compression ratio grows
exponentially as the sequence length increases. The compression of Fibonacci
words are much better than the compression of random sequences, just as
expected. The Fibonacci words compresses orders of magnitudes better than
random data. This already suggests that the performance of the Viterbi
algorithm will be very dependent on the input sequences.  \fxwarning{Discuss
  random sequences of alphabet size 2, DNA, and single character sequences?}

\begin{figure}
  \centering
  \input{figures/compression_ratio.tex}
  \caption{The compression ratio of Fibonacci words, random sequences of
    alphabet size 2, random sequences of alphabet size 4, DNA
    sequences, and single character sequences.}
  \label{fig:compression_ratio}
\end{figure}

\section{Preprocessing}

As described in section~\ref{sec:saving-compr-sequ} the compression of the
input sequence may be done prior to executing the Viterbi or Forward-Backward
algorithms. This is useful if the Viterbi algorithm is run with different
models, as the compression of the sequence does not depend on the model.

The preprocessing that has a theoretical running time of
$O( \left( \lvert\mathcal{O'}\rvert - \lvert{\mathcal{O}}\rvert \right) T)$ as
stated in section~\ref{sec:running-time}. In figure~\ref{fig:pre_viterbi_n} the
running time divided by the length of the sequence, $T$, is shown for random
sequences of varying length. As seen the running time is not linear in $T$,
since the compression ratio is dependent on $T$. Since the size of the new
alphabet $\mathcal{O'}$ grows with the compression ratio, the new alphabet size
is also dependent on $T$. Hence, what is seen in the figure is not surprising;
the running time of the preprocessing is super linear in $T$ and not linear as
the theoretical running time suggests.

\begin{figure}
  \centering
  \input{figures/pre_viterbi_n}
  \caption{zipHMMlib preprocessing time for varying sequence lengths.}
  \label{fig:pre_viterbi_n}
\end{figure}

\section{Viterbi}

This section covers the experiments made in context of the Viterbi
algorithm. First, the theoretical asymptotic running time of the algorithm is
verified to make sure that the running time of the algorithm behaves as
expected. Secondly an implementation of the classical Viterbi algorithm is
compared to the zipHMMlib implementation with compressed both enabled and
disabled.

\subsection{Verification of Theoretical Running Times}
\label{sec:theor-runn-times}

In this section the theoretical running times are verified to make sure that
the implementation runs as expected. The running time has been measured
seperately for the preprocessing and Viterbi with and without backtracking
enabled.

\subsubsection{Running Time Without Backtracking}

The implementation of the Viterbi algorithm supports both computing the Viterbi
path and not doing it. For the two of those there is a difference in the
theoretical time, since computing the path is linear in $T$.

As stated in section~\ref{sec:running-time} the running time without
backtracking is linear in the length of the compressed sequence and cubic in
the number of states.

In figure~\ref{fig:assymptotic_viterbi_n} the running divided by the length of
the compressed sequence is shown for input sequences of varying lengths. As
expected this fraction is close to constant with a slightly decreasing curve,
thus verifying that the algorithm is linear in $T'$.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_n}
  \caption{zipHMMlib running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_n}
\end{figure}

A similar experiment has been conducted to verify that the running time i cubic
in the number of states. This is shown in
figure~\ref{fig:assymptotic_viterbi_k}. Again, as expected the fraction is
decreasing and goes towards being constant.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_k}
  \caption{zipHMMlib running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_k}
\fxnote{Make experiment without compression enabled? Then the running time
  should be only quadratic in the number of states.}
\end{figure}

\subsubsection{Running Time With Backtracking}

In most cases the Viterbi path is needed. Thus, backtracking has to be
done. As discussed in section~\ref{sec:running-time} the running time of the
backtracking depends on the method used. In the following plots both the
running time of the ``normal'' backtracking and the ``space saving''
backtracking is shown.

Both backtracking methods have a running time linear in the length $T$ of the
original sequence. To verify this the running time of the backtracking itself
divided by $T$ for sequences of varying lengths and show in
figure~\ref{fig:assymptotic_viterbi_backtrack_n}. As seen it is indeed
linear.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_backtrack_n}
  \caption{zipHMMlib Path backtracking time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_backtrack_n}
\end{figure}

The normal backtracking should be constant in the number of states $N$, while
the space saving method has a running time is propotinal to $N^2$. The running
time is shown for various number of states in
figure~\ref{fig:assymptotic_viterbi_backtrack_k}. Notice that three curves are
shown. The curve looks as expected for the space saving method, but it turns
out that the running time of the normal backtracking depends on the number of
states. To make the curve not grow, the running time has to be divided by
$N^2$, which does not fit the theoretical running time. This has been
investigated and the reason seems to be that a few more calculations has to be
done to will out the backtracking table. These calculations access matrices of
size $N^2$, and as $N$ becomes larger these matrices may not be in the cache of
the CPU, thus making the computation slower. \fxwarning{Can we see that?}

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_backtrack_k}
  \caption{zipHMMlib Path backtracking time for varying model sizes.}
  \label{fig:assymptotic_viterbi_backtrack_k}
\end{figure}

For completeness the total running time of the Viterbi algorithm including
backtracking is shown in figures~\ref{fig:assymptotic_viterbi_path_n} and
\ref{fig:assymptotic_viterbi_path_k}. The running time is divided by $T$ and
$N^3$ respectively, and as seen this still forms a decreasing curve going
towards a constant.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_path_n}
  \caption{zipHMMlib Path running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_path_n}
\end{figure}

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_path_k}
  \caption{zipHMMlib Path running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_path_k}
\end{figure}

This ends the experiments concerning the theoretical running times of the
Viterbi algorithm itself.

\subsection{Comparing zipHMMlib Viterbi to the Classical Viterbi Algorithm}

First the zipHMMlib Viterbi algorithm is compared to the classical
implementation of the Viterbi algorithm. Recall that the preprocessing may be
saved to disk for later use. For some problems one might only need to run
Viterbi once for a sequence. As described in section~\ref{sec:compr-stopp-crit}
the preprocessesing uses an estimate of how many times the Viterbi algorithm is
run to compress the sequence appropriately. Experiments have been made with one
execution of Viterbi and with 500 executions of Viterbi. In total, six
variations of the Viterbi algorithm have been measured:
\begin{description}
% \item[Uncompressed] that does not compress the sequence and only find the
%   loglikelihood of the most probable path,
% \item[Uncompressed Path] that also backtracks to find the most likely path,
% \item[Uncompressed Path Space Saving] that also saves memory in the
%   backtracking process,
\item[Viterbi 1] that compresses the sequence and only computes the
  loglikelihood,
\item[Viterbi 1 Path] that also backtracks,
\item[Viterbi 1 Path Space Saving] that also saves memory on backtracking.
\item[Viterbi 500] that compresses the sequence and computes the
  loglikelihood 500 times,
\item[Viterbi 500 Path] that also backtracks 500 times,
\item[Viterbi 500 Path Space Saving] that also saves memory on backtracking.
\end{description}

In the following plots a ``speedup factor'' is shown on the y-axis. The speedup
factor is calculated as the running time of the classical algorithm (with or
out without backtracking enabled) divided by the running time of algorithm
being measured. Hence, any speedup factor larger than one means that the
algorithm being measured is faster than the classical algorithm.

\subsubsection{Random Data}

The first experiments have been made using random sequences.

In figures~\ref{fig:compressed_1_speedup_vs_sequence_length}
and~\ref{fig:compressed_500_speedup_vs_sequence_length} the speedup factor for
the Viterbi algorithms is shown for sequences of varying length. \fxnote{Should
  this be avoided by the compression stopping criterion?}

As expected, computing the path limits the speedup increasingly as the length
of the input sequence becomes larger.
\fxwarning{Elaborate.}

\begin{figure}
  \centering
  \input{figures/compressed_1_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi 1 algorithms of the total
    running time including preprocessing for sequences of varying lengths using
    a HMM with 16 states.}
  \label{fig:compressed_1_speedup_vs_sequence_length}
\fxnote{It should not be decreasing when the compression criterion is set for
  Viterbi explicitly.}
\end{figure}

\begin{figure}
  \centering
  \input{figures/compressed_500_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi 500 algorithms of the total
    running time including preprocessing for sequences of varying lengths using
    a HMM with 16 states.}
  \label{fig:compressed_500_speedup_vs_sequence_length}
\end{figure}

Finally, instead of viewing the speedup factor as a function of the sequence
length, the model size is used. Recall that the running time of Viterbi is
$O(M' N^3 + N^2 T')$. Hence, the cubic factor might become a problem as $N$
grows. The speedup factor as function of model size $N$ is plotted in
figures~\ref{fig:speedup_vs_k} and~\ref{fig:speedup_vs_k2}. As expected the
speedup continues growing until a certain number of states and then starts to
become smaller again. It is noticeable though that the good speedup factors are
gained for quite large models of hundreds of states.

\begin{figure}
  \centering
  \input{figures/speedup_vs_k.tex}
  \caption{The speedup factor for varying model sizes using the Viterbi 1
    algorithms.}
  \label{fig:speedup_vs_k}
\end{figure}

\begin{figure}
  \centering
  \input{figures/speedup_vs_k2.tex}
  \caption{The speedup factor for varying model sizes using the Viterbi 500
    algorithms.}
  \label{fig:speedup_vs_k2}
\end{figure}

In conclusion the zipHMMlib implementation of Viterbi obtains a speedup on
random data given the right circumstances. In all cases a much larger speedup
is obtained if the Viterbi path is not needed. The Viterbi Path Space Saving
algorithm is slower noticeably slower than the Viterbi Path algorithm. Which
of the algorithms to use is a tradeoff between memory usage and speed of the
program. If one only needs to run the algorithm once, compression should be
either enabled or disabled depending on whether the path is requested or
not. If the Viterbi algorithm is run for multiple models, a speedup is gained
by compressing the sequence. If the model becomes too large though the
algorithm fails at gaining a speedup.

\subsubsection{Fibonacci Words}

Even though good speedup factors were gained for random data, it is expected
that much greater speedups may be obtained by using repetitive
sequences. Thus, experiments similar to the ones for random data has been
conducted.

\begin{figure}
  \centering
  \input{figures/fib_compressed_1_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi 1 algorithms of the total
    running time including preprocessing for Fibonacci sequences of varying
    lengths using a HMM with 16 states.}
  \label{fig:fib_compressed_1_speedup_vs_sequence_length}
\end{figure}

\begin{figure}
  \centering
  \input{figures/fib_compressed_500_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi 500 algorithms of the total
    running time including preprocessing for Fibonacci sequences of varying
    lengths using a HMM with 16 states.}
  \label{fig:fib_compressed_500_speedup_vs_sequence_length}
\end{figure}


This ends the section experimenting with the Viterbi algorithm. It is concluded
that the compression of sequences does indeed speed up the execution time of
the Viterbi algorithm. But to gain a speed up, some restrictions are
required. If the number of states in the model becomes too large, a speed up is
gained. Secondly, a speedup is not necessarily gained if the Viterbi algorithm
is only run a single time for a sequence, since the preprocessing/compression of
the sequence takes too much time relatively to the execution of the Viterbi
algorithm. This highly depends on the sequence though. For the very repetitive
Fibonacci words, a speedup was gained even by running the Viterbi algorithm once.

\section{Posterior Decoding}

This section contains all experiments performed on the posterior decoding
algorithm. As described in section~\ref{sec:probl-expl-repet}, no efficient way
of exploiting sequence repetitions has been found. Nevertheless experiments
have been made to see whether a minor speedup can be obtained by computing the
posterior decoding in the matrix multiplications framework. Hence, only one of
the four variations introduced in the Viterbi experiments makes sense here,
namely the Uncompressed Path variation, as the path is always wanted when doing
posterior decoding.

The section is split into two parts. The first part compares the actual
running time to the theoretical running time to verify that the implementation
of the algorithm satisfies the theory. The second part compares it to the
classical implementation of posterior decoding.

\subsection{Asymptotic Running Time}

Like in section~\ref{sec:theor-runn-times}, the theoretical compared the the
actual running times experiments have been made using random data. The
theoretical running time is $O(M N^3 + TN^2)$. Since $M$ is very small compared
to $T$ is expected that the first term vanishes. Hence, for this experiment the
theoretical running time $O(TN^2)$ is assumed. This of course only makes sense
if $N$ is not too large, but as seen in the plots the assumption of a
theoretical running time of $O(TN^2)$ does make sense for these experiments.

First, the running time compared to the sequence length is shown in
figure~\ref{fig:posterior_n}. A model with 16 states has been used. As expected
the running time is slightly decreasing going towards constant.

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_n.tex}
  \caption{The running time of posterior decoding is linear in the size of the
    input sequence.}
  \label{fig:posterior_n}
\end{figure}

Secondly, in figure~\ref{fig:posterior_k} the running time for an increasing
number of states is shown. A random sequence of length 10000 has been used. It
can be seen that the running time is indeed quadratic in the numbers of states.

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_k.tex}
  \caption{The running time of posterior decoding is quadratic in the number of
    states.}
  \label{fig:posterior_k}
\end{figure}

It is concluded that the actual running time of the algorithm follows the
theoretical.

\subsection{Comparison to the Classical Algorithm}

Now the algorithm is compared the classical algorithm. In
figure~\ref{fig:posterior_speedup_vs_sequence_length} the speedup factor as
function of the sequence length is shown. A model with 16 states was used for
the experiment. In all tested cases the algorithm is a bit faster than the
classical implementation. There is a tendency that the speedup slowly becomes
larger as the input sequence grows. Speedup factors between 1 and 4 are gained
in this experiment. This may change if the number of states in the model is
different.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for sequences of varying lengths using a HMM
    with 16 states.}
  \label{fig:posterior_speedup_vs_sequence_length}
\end{figure}

Looking at the speedup factor as function of the number of states in
figure~\ref{fig:posterior_speedup_vs_k}, it is seen that for small models it is
below 1. This means that the algorithm is slower than the classical posterior
decoding algorithm. However, as the number of states in the model increases the
speedup factor quickly becomes larger than 1.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_k.tex}
  \caption{The speed up factor as function of the model size using a sequence
    of length 10000.}
  \label{fig:posterior_speedup_vs_k}
\end{figure}

These experiments ends the section of posterior decoding. Overall the algorithm
has a running time that is very comparable to the classical implementation of
posterior decoding. For scenarios with large models and sequences the zipHMMlib
is faster whereas the classical algorithm is a better for small models.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
