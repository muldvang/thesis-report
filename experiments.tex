% chktex-file 24
% chktex-file 44
\chapter{Experiments}
\label{cha:experiments}

This chapter covers description and discussion of all experiments performed in
this thesis. First, the experimental setup is described, followed by the data
used, and then the Viterbi and posterior algorithms are discussed in turn.

The experiments have been run on a PC with an Intel Xeon W3550 $3.07$ GHz CPU
and 4 GB RAM running GNU/Linux 3.13. Each point in the plots in this chapter is
formed by the mean of multiple runs. In the case of random sequences, 20 random
sequences of the same length has been generated and the mean of the measure
calculated. In case e.g.\ Fibonacci words, only a single sequence of a
specified length exist, so in this case the experiment has been performed five
times and the mean calculated to even out fluctuations in the
measure. Furthermore, the standard deviation has been calculated and is shown
as error bars in the plots.

\section{Data}
\label{sec:data}

Hidden Markov models have succesfully been applied to various kinds of
data. \fxwarning{Reference} As thes thesis is written with no specific kind of
data in mind, aside from repetitive data of course, the experiments primarily
use random data that does not contain many repetitions and Fibonacci words that
is a highly repetitive kind of sequence. \fxnote{DNA data?}

The models used is random, fully connected models, that is there is a
transition from a state to any other state. The emission probabilities are
random too. 10 random models with $2, 4, 8, \dots, 1024$ states have been
generated.

The input sequences used is now described in turn.

\begin{description}
\item[Random sequences] are generated by one of the random models by making
  transitions and emissions according to the parameters of that model. The
  generated sequences have length $10^3$ to approximately $10^7$.
\item[Fibonacci words] are generated in a way very similar to how Fibonacci
  numbers are generated by concatenating the two previous words instead of
  adding them. Let $S_0$ be ``0'' and $S_1$ be ``01''. Now
  $S_n=S_{n-1}S_{n-2}$. In contrast to the random sequences, Fibonacci words
  contain many repetitions and compress very well using byte-pair
  encoding. \fxwarning{Reference.}
\item[DNA sequences] \fxerror{Write something or delete.}
\end{description}

\section{Compression Ratio}
\label{sec:compression-ratio}

The running time of the Viterbi algorithm implemented in zipHMMlib is highly
dependent on the size of the compressed sequence. As mentioned in
section~\ref{sec:compr-stopp-crit} it is posible to compress the sequence to a
single character, eventhough this is not feasible in terms of performance. To
see how well the different kinds of data compresses when used by zipHMMlib the
compression ratio for all the sequences described above have been measured. The
compression ratio is defined as
$\frac{\text{original size}}{\text{compressed size}}$. Hence, the greater
compression ratio the better.

As seen in figure~\ref{fig:compression_ratio} the compression ratio of
Fibonacci words are much better than the compression of random sequences, just
as expected. The Fibonacci words compresses orders of magnitudes better than
random data. This already suggests that the performance of the Viterbi
algorithm will be very dependent on the input sequences.  \fxwarning{Discuss
  random sequences of alphabet size 2, DNA, and single chracter sequences?}

\begin{figure}[H]
  \centering
  \input{figures/compression_ratio.tex}
  \caption{The compression ratio of Fibonacci words, random sequences of
    alphabet size 2, random sequences of alphabet size 4, DNA
    sequences, and single character sequences.}
  \label{fig:compression_ratio}
\end{figure}

\section{Viterbi}
\label{sec:viterbi}

This section covers the experiments made in context of the Viterbi
algorithm. First, the theoretical assymptotic running time of the algorithm is
verified to make sure that the running time of the algorithm behaves as
expected. Secondly an implementation of the classical Viterbi algorithm is
compared to the zipHMMlib implementation with compressed both enabled and
disabled.

\subsection{Verification of Theoretical Running Times}
\label{sec:theor-runn-times}

\subsubsection{Preprocessing}
\label{sec:preprocessing}

\begin{figure}[H]
  \centering
  \input{figures/pre_viterbi_n}
  \caption{zipHMMlib preprocessing time for varying sequence lengths.}
  \label{fig:pre_viterbi_n}
\end{figure}

% \begin{figure}[H]
%   \centering
%   \input{figures/pre_viterbi_k}
%   \caption{zipHMMlib preprocessing time for varying model sizes.}
%   \label{fig:pre_viterbi_k}
% \end{figure}

\subsubsection{Running Time Without Backtracking}
\label{sec:running-time-without}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_n}
  \caption{zipHMMlib running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_k}
  \caption{zipHMMlib running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_k}
\end{figure}

\subsubsection{Running Time With Backtracking}
\label{sec:running-time-with-1}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_backtrack_n}
  \caption{zipHMMlib Path backtracking time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_backtrack_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_backtrack_k}
  \caption{zipHMMlib Path backtracking time for varying model sizes.}
  \label{fig:assymptotic_viterbi_backtrack_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_path_n}
  \caption{zipHMMlib Path running time for varying sequence lengths.}
  \label{fig:assymptotic_viterbi_path_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_viterbi_path_k}
  \caption{zipHMMlib Path running time for varying model sizes.}
  \label{fig:assymptotic_viterbi_path_k}
\end{figure}

\subsection{Comparing zipHMMlib to the original Viterbi algorithm.}
\label{sec:comp-ziphmml-orig}

First we comparing zipHMMlib to the original implementation of the Viterbi
algorithm. Recall that the preprocessesing may be saved to disk for later
use. For some problems one might only need to run Viterbi once for a
sequence. In  this case we should look at the running time including
preprocessesing. It might be better to not spend any time compressing the
sequence. In the plots below the time of computing Viterbi on a noncompressed
sequence is also shown.

% \begin{figure}[H]
%   \centering
%   \input{figures/speedup_vs_complexity.tex}
%   \caption{Running time vs.\ sequence complexity using sequences of length $10^6$.}
%   \label{fig:speedup_vs_complexity}
% \end{figure}

\subsubsection{Random Data}
\label{sec:random-data}

Including preprocessesing time:
\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_sequence_length.tex}
  \caption{The speed up factor of the total running time include preprocessing
    for sequences of varying lengths using a HMM with 16 states.}
  \label{fig:speedup_vs_sequence_length}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_k.tex}
  \caption{Running time vs.\ model size.}
  \label{fig:speedup_vs_k}
\end{figure}

Excludeing preprocessesing time:
\begin{figure}[H]
  \centering
  \input{figures/speedup_vs_sequence_length2.tex}
  \caption{The speed up factor of the running time excluding preprocessing for
    sequences of varying lengths using a HMM with 16 states.}
  \label{fig:speedup_vs_sequence_length2}
\end{figure}

\subsubsection{Fibonacci Words}
\label{sec:fibonacci-words-1}

\begin{figure}[H]
  \centering
  \input{figures/fib_speedup_vs_sequence_length.tex}
  \caption{The speed up factor of the total running time include preprocessing
    for Fibonacci words of varying lengths using a HMM with 16 states.}
  \label{fig:fib_speedup_vs_sequence_length}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/fib_speedup_vs_sequence_length2.tex}
  \caption{The speed up factor of the running time excluding preprocessing for
    Fibonacci words of varying lengths using a HMM with 16 states.}
  \label{fig:fib_speedup_vs_sequence_length2}
\end{figure}

\section{Posterior Decoding}
\label{sec:posterior-decoding}

\subsection{Assymptotic Running Time}
\label{sec:assympt-runn-time}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_posterior_n.tex}
  \caption{The running time of posterior decoding is linear in the size of the
    input sequence.}
  \label{fig:posterior_n}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/assymptotic_posterior_k.tex}
  \caption{The running time of posterior decoding is quadratic in the number of
    states.}
  \label{fig:posterior_k}
\end{figure}

\subsection{Comparison to the Naive Algorithm}
\label{sec:comp-naive-algor}

\begin{figure}[H]
  \centering
  \input{figures/posterior_speedup_vs_k.tex}
  \caption{The speed up factor as function of the model size using a sequence
    of length 100000.}
  \label{fig:posterior_speedup_vs_k}
\end{figure}

\begin{figure}[H]
  \centering
  \input{figures/posterior_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for sequences of varying lengths using a HMM
    with 16 states.}
  \label{fig:posterior_speedup_vs_sequence_length}
\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
