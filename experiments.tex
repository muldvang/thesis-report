% chktex-file 24
% chktex-file 44
\chapter{Experiments}

This chapter covers description and discussion of all experiments performed in
this thesis. First, the experimental setup is described, followed by the data
used, and then the Viterbi and posterior algorithms are discussed in turn.

\section{Data}

Hidden Markov models have successfully been applied to various kinds of
data. \fxwarning{Reference} As this thesis is written with no specific kind of
data in mind, aside from repetitive data of course, the experiments primarily
use random data that does not contain many repetitions and Fibonacci words that
is a highly repetitive kind of sequence. The two types of sequences used is now
described in turn.

\begin{description}
\item[Random sequences] are generated by one of the models (described below) by
  making random transitions and emissions according to the probabilities
  specified by the parameters of that model. The generated sequences varies in
  length from $10^3$ to approximately $10^7$.
\item[Fibonacci words] are generated in a way very similar to how Fibonacci
  numbers are generated by concatenating the two previous words instead of
  adding them. Let $S_0$ be ``0'' and $S_1$ be ``01''. Now
  $S_n=S_{n-1}S_{n-2}$. In contrast to the random sequences, Fibonacci words
  contain many repetitions and compress very well using byte-pair
  encoding. \fxwarning{Reference.}
\end{description}

The models used are random, fully connected models, that is there is a
transition from a state to any other state. The emission probabilities are
random too. The alphabet size has been kept constant at four. In general, the
larger the alphabet, the wrose compression ratio is expected. The size of the
models, i.e.\ the number of states have been varied from 2 to 512.

\section{Experimental Setup}

The experiments have been run on a PC with an Intel Xeon W3550 $3.07$ GHz CPU
and 4 GB RAM running GNU/Linux 3.13. Each point in the plots in this chapter is
formed by the mean of multiple runs. In the case of random sequences, 20 random
sequences of the same length has been generated and the mean of the measure
calculated. In case e.g.\ Fibonacci words, only a single sequence of a
specified length exist, so in this case the experiment has been performed five
times and the mean calculated to even out fluctuations in the
measure. Furthermore, the standard deviation has been calculated and is shown
as error bars in the plots.

\section{Compression Ratio}
\label{sec:compression-ratio}

The running time of the Viterbi algorithm implemented in zipHMMlib is highly
dependent on the size of the compressed sequence. As mentioned in
section~\ref{sec:compr-stopp-crit} it is possible to compress the sequence to a
single character, even though this is not a good idea in terms of
performance. To see how well the different kinds of data compresses when used
by zipHMMlib the compression ratio for all the sequences described above have
been measured. The compression ratio is defined as
$\frac{\text{original size}}{\text{compressed size}}$. Hence, the greater
compression ratio the better. Recall, that the amount of compression depends on
the estimate of how many times the algorithm is run after compressing the
sequence. For this experiment an estimate of $e = 500$ have been provided to
the program.

As seen in figure~\ref{fig:compression_ratio} the compression ratio grows
exponentially for the Fibonacci words as the sequence length increases, whereas
is it almost constant at a ratio of four for randon sequences. The compression
of Fibonacci words are much better than the compression of random sequences,
just as expected. This already suggests that the performance of the Viterbi
algorithm will be very dependent on the input sequences. Just to verify that
the Fibonacci sequences is very repetitive unary sequences have also been
compressed, and as seen the compression of these two types is very comparable.

\begin{figure}
  \centering
  \input{figures/compression_ratio.tex}
  \caption{The compression ratio of Fibonacci words, random sequences of
    alphabet size 2, random sequences of alphabet size 4, DNA
    sequences, and single character sequences.}
  \label{fig:compression_ratio}
\end{figure}

\section{Preprocessing}

As described in section~\ref{sec:saving-compr-sequ} the compression of the
input sequence may be done prior to executing the Viterbi or Forward-Backward
algorithms. This is useful if the Viterbi algorithm is run with different
models, as the compression of the sequence does not depend on the model.

The preprocessing that has a theoretical running time of
$O( \left( \lvert\mathcal{O'}\rvert - \lvert{\mathcal{O}}\rvert \right) T)$ as
stated in section~\ref{sec:running-time}. In figure~\ref{fig:pre_viterbi_T} the
running time divided by the length of the sequence, $T$, is shown for random
sequences of varying length. As seen the running time is not linear in $T$,
since the compression ratio is dependent on $T$. Since the size of the new
alphabet $\mathcal{O'}$ grows with the compression ratio, the new alphabet size
is also dependent on $T$. Hence, what is seen in the figure is not surprising;
the running time of the preprocessing is super linear in $T$ and not linear as
the theoretical running time suggests.

\begin{figure}
  \centering
  \input{figures/pre_viterbi_T}
  \caption{Viterbi preprocessing time for varying sequence lengths.}
  \label{fig:pre_viterbi_T}
\end{figure}

\section{Viterbi}

This section covers the experiments made in context of the Viterbi
algorithm. First, the theoretical asymptotic running time of the algorithm is
verified to make sure that the running time of the algorithm behaves as
expected. Secondly an implementation of the classical Viterbi algorithm is
compared to the zipHMMlib implementation with compressed both enabled and
disabled.

\subsection{Verification of Theoretical Running Times}
\label{sec:theor-runn-times}

In this section the theoretical running times are verified to make sure that
the implementation runs as expected. The running time has been measured for
three variations of the Viterbi algorithm corresponding to the backtracking
discussed in section~\ref{sec:backtracking}. They are named as follows.
\begin{description}
\item[Viterbi\textsubscript{L}] that compresses the sequence and only computes the
  loglikelihood,
\item[Viterbi\textsubscript{P}] that also backtracks,
\item[Viterbi\textsubscript{PM}] that also saves memory on backtracking.
\end{description}

The next two sections verifies that the implementation follows the theoretical
running times.

\subsubsection{Sequence Length}

The implementation of the Viterbi algorithm supports both computing the Viterbi
path and not doing it. For the two of those there is a difference in the
theoretical time, since computing the path is linear in $T$.

In figure~\ref{fig:assymptotic_viterbi_T} the running divided by the length of
the compressed sequence $T'$ is shown for input sequences of varying
lengths. As expected this fraction is close to constant with a slightly
decreasing curve for Viterbi\textsubscript{L}, whereas it starts increasing for Viterbi\textsubscript{P} and
Viterbi\textsubscript{PM}. Thus as expected Viterbi\textsubscript{L} is linear in $T'$, while
the backtracking makes it superlinear in $T'$.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_T}
  \caption{The running time of Viterbi as a function of the compressed sequence
    length $T'$. The running time of Viterbi\textsubscript{L} is linear in
    $T'$ whereas Viterbi\textsubscript{P} and Viterbi\textsubscript{PM} are
    superlinear.}
  \label{fig:assymptotic_viterbi_T}
\end{figure}

Both backtracking methods have a theoretical running time linear in the length
of the original sequence $T$. To verify this the running time of the
backtracking itself divided by $T$ for sequences of varying lengths and shown in
figure~\ref{fig:assymptotic_viterbi_backtrack_T}. As seen it is indeed linear.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_backtrack_T}
  \caption{The running time of Viterbi as a function of the original sequence
    length $T$. The running time of all three variations of the Viterbi
    algorithm are linear in $T$.}
  \label{fig:assymptotic_viterbi_backtrack_T}
\end{figure}

\subsubsection{Model Size}

The theoretical running times of all three variation of the Viterbi algorithm
are cubic in the number of states $N$. To verify this the running time divided
by $N^3$ is shown in figure~\ref{fig:assymptotic_viterbi_backtrack_N}. As seen
the points of all three algorithms form a decreasing curve verifying that the
running time is cubic in $N$.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_backtrack_N}
  \caption{The running time of Viterbi as a function of the number of hidden
    states in the model $N$. The running time of all three variations of the Viterbi
    algorithm are cubic in $N$.}
  \label{fig:assymptotic_viterbi_backtrack_N}
\end{figure}

Recall that the running time is cubic in $N$ is due to the computation of the
$C$ matrices. The multiplication of the matrices can be done in time
proportional to $N^2$ as stated in section~\ref{sec:running-time}. When the
size of the new alphabet $M'$ is small compared to the length of the compressed
seuence, the $N^3$ term vanishes in practice when compared to the $N^2$
term. Hence, it may be seen that the running time of the matrix multiplication
(and the backtracking) is propotional to $N^2$ by not compressing the
sequence. This is shown in figure~\ref{fig:assymptotic_viterbi_N} where
$\lvert M' \rvert = 4$ and $\lvert T' \rvert = 10.000$.

\begin{figure}
  \centering
  \input{figures/assymptotic_viterbi_N}
  \caption{The running time of Viterbi as a function of the number of hidden
    states in the model $N$. If the input sequences are not compressed, the
    running time of all three variations of the Viterbi algorithm are quadratic
    in $N$.}
  \label{fig:assymptotic_viterbi_N}
\end{figure}

% This ends the experiments concerning the theoretical running times of the
% Viterbi algorithm itself.

\subsection{Comparing zipHMMlib Viterbi to the Classical Viterbi Algorithm}

First the zipHMMlib Viterbi algorithm is compared to the classical
implementation of the Viterbi algorithm. Recall that the preprocessing may be
saved to disk for later use. For some problems one might only need to run
Viterbi once for a sequence. As described in section~\ref{sec:compr-stopp-crit}
the preprocessesing uses an estimate of how many times the Viterbi algorithm is
run to compress the sequence appropriately. Experiments have been made with one
and with 500 executions of Viterbi. In total, six variations of the Viterbi
algorithm have been measured:
\begin{description}
% \item[Uncompressed] that does not compress the sequence and only find the
%   loglikelihood of the most probable path,
% \item[Uncompressed Path] that also backtracks to find the most likely path,
% \item[Uncompressed Path Memory] that also saves memory in the
%   backtracking process,
\item[Viterbi\textsubscript{L} 1] that compresses the sequence and only computes the
  loglikelihood once,
\item[Viterbi\textsubscript{P} 1] that also backtracks,
\item[Viterbi\textsubscript{PM} 1] that also saves memory on backtracking.
\item[Viterbi\textsubscript{L} 500] that compresses the sequence and computes the
  loglikelihood 500 times,
\item[Viterbi\textsubscript{P} 500] that also backtracks 500 times,
\item[Viterbi\textsubscript{PM} 500] that also saves memory on backtracking.
\end{description}

The algorithms are compared to their respective counterpart of the classical
algorithm, e.g.\ for Viterbi\textsubscript{L} algorithms the classical
algorithm without backtracking has been used. In the following plots a
``speedup factor'' is shown on the y-axis. The speedup factor is calculated as
the running time of the classical algorithm (with or out without backtracking
enabled) divided by the running time of algorithm being measured, including the
time it takes to do the compression. Hence, any speedup factor larger than one
means that the algorithm being measured is faster than the classical algorithm.

\subsubsection{Random Data}

The first experiments have been made using random sequences.

In figures~\ref{fig:compressed_1_speedup_vs_sequence_length}
and~\ref{fig:compressed_500_speedup_vs_sequence_length} the speedup factor for
the Viterbi 1 algorithms and Viterbi 500 algorithms is shown for sequences of
varying length. A model with 16 states was used for this experiment. As seen
the speedup factor decreases as the length of the sequence increases. This is
due to the preprocessing that takes more time for longer
sequences. \fxnote{Should this be avoided by the compression stopping
  criterion?} \fxwarning{Discuss that the speedup factor of the backtracking
  algorithms has a greater speedup factor than the likelihood computing.} This
is not seen for the Viterbi 500 algorithms, where the speedup factor keeps
increasing for larger sequences. This is because the preprocessesing does not
dominate the running time as it does for the Viterbi 1 algorithms.

As expected, computing the path limits the speedup factor since the running
time is linear in $T$ instead of $T'$. The Viterbi\textsubscript{PM} does not
gain a speedup as good as the Viterbi\textsubscript{P}. \fxwarning{Elaborate.}

\begin{figure}
  \centering
  \input{figures/compressed_1_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 1 algorithms of the total
    running time for sequences of varying lengths using
    a HMM with 16 states.}
  \label{fig:compressed_1_speedup_vs_sequence_length}
\fxnote{It should not be decreasing when the compression criterion is set for
  Viterbi explicitly.}
\end{figure}

\begin{figure}
  \centering
  \input{figures/compressed_500_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 500 algorithms of the total
    running time for sequences of varying lengths using
    a HMM with 16 states.}
  \label{fig:compressed_500_speedup_vs_sequence_length}
\end{figure}

Finally, instead of viewing the speedup factor as a function of the sequence
length, the model size is used. Recall that the running time of Viterbi is
$O(M' N^3 + N^2 T')$. Hence, the cubic factor might become a problem as $N$
grows. The speedup factor as function of model size $N$ is plotted in
figures~\ref{fig:speedup_vs_N} and~\ref{fig:speedup_vs_N2}. As expected the
speedup continues growing until a certain number of states and then starts to
become smaller again. It is noticeable though that the good speedup factors are
gained for quite large models of hundreds of states.

\begin{figure}
  \centering
  \input{figures/speedup_vs_N.tex}
  \caption{The speedup factor for varying model sizes using the Viterbi\textsubscript{L} 1
    algorithms.}
  \label{fig:speedup_vs_N}
\end{figure}

\begin{figure}
  \centering
  \input{figures/speedup_vs_N2.tex}
  \caption{The speedup factor for varying model sizes using the Viterbi\textsubscript{L} 500
    algorithms.}
  \label{fig:speedup_vs_N2}
\end{figure}

In conclusion the zipHMMlib implementation of Viterbi obtains a speedup on
random data given the right circumstances. In all cases a much larger speedup
is obtained if the Viterbi path is not needed. The Viterbi\textsubscript{PM}
algorithm is slower noticeably slower than the Viterbi\textsubscript{P} algorithm. Which
of the algorithms to use is a tradeoff between memory usage and speed of the
program. If one only needs to run the algorithm once, compression should be
either enabled or disabled depending on whether the path is requested or
not. If the Viterbi algorithm is run for multiple models, a speedup is gained
by compressing the sequence. If the model becomes too large though the
algorithm fails at gaining a speedup.

\subsubsection{Fibonacci Words}

Even though good speedup factors were gained for random data, it is expected
that much greater speedups may be obtained by using repetitive
sequences. Thus, experiments similar to the ones for random data has been
conducted. In figures~\ref{fig:fib_compressed_1_speedup_vs_sequence_length}
and~\ref{fig:fib_compressed_500_speedup_vs_sequence_length} the speedup factor
is shown for sequences of varying length using 1 or 500 runs respectively.

\begin{figure}
  \centering
  \input{figures/fib_compressed_1_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 1 algorithms of the total
    running time for Fibonacci sequences of varying
    lengths using a HMM with 16 states.}
  \label{fig:fib_compressed_1_speedup_vs_sequence_length}
\end{figure}

\begin{figure}
  \centering
  \input{figures/fib_compressed_500_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for the Viterbi\textsubscript{L} 500 algorithms of the total
    running time for Fibonacci sequences of varying
    lengths using a HMM with 16 states.}
  \label{fig:fib_compressed_500_speedup_vs_sequence_length}
\end{figure}

As seen, the speedup factors are much greater than for random data. For random
data the computation of the Viterbi path limited the speedup factor. This
limitation only becomes larger for Fibonacci sequences. This may e.g.\ be seen
in figure~\ref{fig:fib_compressed_500_speedup_vs_sequence_length}, where the
computation of the Viterbi path makes the computation orders of magnitudes
slower than just computing the likelihood. However, the speedup factor for the
Viterbi\textsubscript{P} algorithms still perform better on Fibonacci sequences than on
random sequences.

\subsubsection{Future Work}

To get an impression of where the bottleneck of the algorithm is, the running
time of the preprocessing and the execution of the Viterbi algorithm has been
measured. To keep the number of graphs at a reasonable level, the experiment
has only been made using random data and only the Viterbi\textsubscript{L}
algorithm have been used. Similar results may be obtained using the
Viterbi\textsubscript{P} and Viterbi\textsubscript{PM} algorithms. Again, for
the Viterbi 1 the preprocessing has been done and then Viterbi has been
executed once. For Viterbi 500 the preprocessing has been done follow by 500
executions of Viterbi. The result can be seen in
figure~\ref{fig:pre_vs_running}.

\begin{figure}
  \centering
  \begin{subfigure}{0.5\textwidth}
    \centering \input{figures/viterbi_pre_vs_running_one.tex}
    \caption{Viterbi\textsubscript{L} 1}
  \end{subfigure}%
  \begin{subfigure}{0.5\textwidth}
    \centering \input{figures/viterbi_pre_vs_running_many.tex}
    \caption{Viterbi\textsubscript{L} 500}
  \end{subfigure}

  % \begin{subfigure}{0.5\textwidth}
  %   \centering \input{figures/viterbi_pre_vs_running_one_path_memory.tex}
  %   \caption{Viterbi\textsubscript{PM} 1}
  % \end{subfigure}%
  % \begin{subfigure}{0.5\textwidth}
  %   \centering \input{figures/viterbi_pre_vs_running_many_path_memory.tex}
  %   \caption{Viterbi\textsubscript{PM} 500}
  % \end{subfigure}
  \ref{named}
  \caption{Fraction of the running time spent on preprocessing and Viterbi
    algorithm for input sequences of varying length. For the Viterbi 1
    experiment the largest part of the running times comes from the
    preprocessing, whereas the the fraction of time spent on preprocessing for
    the Viterbi 500 experiment is almost zero.}
  \label{fig:pre_vs_running}
\end{figure}

For the Viterbi\textsubscript{L} 1 experiment it is seen that a major part of the running time
is spent on the preprocessing. However, as the number of executions is
increased the time of the preprocessing vanishes. In terms of performance
gains, making the preprocessing more efficient will only give a gain in speedup
when the number of executions of the Viterbi algorithm is slow, while higher
efficiency in the Viterbi algorithm or in the effectiveness of the compression
will make the running time smaller for many executions.

\subsubsection{Conclusion}

This ends the section experimenting with the Viterbi algorithm. It is concluded
that the compression of sequences does indeed speed up the execution time of
the Viterbi algorithm. The best results are of course found for repetitive data
with the Viterbi algorithm run multiple times. Speedup factors in the order of
thousands were found, so for these kind of problems this method has a great
potential. I other scenarios with random data and only a few hidden states a
speedup factor much lower is obtained such that the execution only is a couples
of times faster.

\section{Posterior Decoding}

This section contains all experiments performed on the posterior decoding
algorithm. As described in section~\ref{sec:probl-expl-repet}, no efficient way
of exploiting sequence repetitions has been found. Nevertheless experiments
have been made to see whether a minor speedup can be obtained by computing the
posterior decoding in the matrix multiplications framework given by
zipHMMlib. In contrast to the previous section with multiple variations of the
Viterbi algorithm, only a single variation makes sense here: the input sequence
is not compressed and the path is computed.

The section is split into two parts. The first part compares the actual
running time to the theoretical running time to verify that the implementation
of the algorithm satisfies the theory. The second part compares it to the
classical implementation of posterior decoding.

\subsection{Asymptotic Running Time}
\label{sec:asympt-runn-time}

Like in section~\ref{sec:theor-runn-times}, the theoretical compared the the
actual running times experiments have been made using random data. The
theoretical running time is $O(M N^3 + TN^2)$. Since $M$ is very small compared
to $T$ is expected that the first term vanishes. This was already seen for the
Viterbi algorithm in section~\ref{sec:theor-runn-times}. Hence, for this
experiment the theoretical running time $O(TN^2)$ is assumed. This of course
only makes sense if $N$ is not too large, but as seen in the plots the
assumption of a theoretical running time of $O(TN^2)$ does make sense for these
experiments.

First, the running time compared to the sequence length is shown in
figure~\ref{fig:posterior_T}. A model with 16 states has been used. As expected
the running time is decreasing going towards constant.

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_T.tex}
  \caption{The running time of posterior decoding is linear in the size of the
    input sequence.}
  \label{fig:posterior_T}
\end{figure}

Secondly, in figure~\ref{fig:posterior_N} the running time for an increasing
number of states is shown. A random sequence of length $10.000$ has been used. It
can be seen that the running time is indeed quadratic in the numbers of states
in practice.

\begin{figure}
  \centering
  \input{figures/assymptotic_posterior_N.tex}
  \caption{The running time of posterior decoding is quadratic in the number of
    states.}
  \label{fig:posterior_N}
\end{figure}

It is concluded that the actual running time of the algorithm follows the
theoretical.

\subsection{Comparison to the Classical Algorithm}

Now the algorithm is compared the classical algorithm. In
figure~\ref{fig:posterior_speedup_vs_sequence_length} the speedup factor as
function of the sequence length is shown. Random sequences and a model with 16
states was used for the experiment. In all tested cases the algorithm is a bit
faster than the classical implementation. There is a tendency that the speedup
slowly becomes larger as the input sequence grows. Speedup factors between 1
and 5 are gained in this experiment. This may change if the number of states in
the model is different.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_sequence_length.tex}
  \caption{The speed up factor for sequences of varying lengths using a HMM
    with 16 states.}
  \label{fig:posterior_speedup_vs_sequence_length}
\end{figure}

Looking at the speedup factor as function of the number of states in
figure~\ref{fig:posterior_speedup_vs_N}, it is seen that for small models it is
below 1. This means that the algorithm is slower than the classical posterior
decoding algorithm. However, as the number of states in the model increases the
speedup factor quickly becomes larger than 1, due to the matrix multiplications
being very efficient due to the BLAS framework.

\begin{figure}
  \centering
  \input{figures/posterior_speedup_vs_N.tex}
  \caption{The speed up factor as function of the model size using a sequence
    of length 10000.}
  \label{fig:posterior_speedup_vs_N}
\end{figure}

These experiments ends the section of posterior decoding. Overall the algorithm
has a running time that is very comparable to the classical implementation of
posterior decoding. For scenarios with large models and sequences the zipHMMlib
is faster whereas the classical algorithm is a better for small models.

\section{Subsequence Posterior Decoding}

In this section experiments on the subsequence posterior decoding is
made. Recall that subsequence posterior decoding also takes two indeces $i$ and
$j$ as input an only returns the posterior decoding for the subsequence
$Y_{i:j}$.

Recall from section~\ref{sec:running-time-2} that the algorithm has a
theoretical running time of $O(M' N^3 + N^2 T' + N^2 (j - i + \delta))$,
assuming the input sequence has already been compressed. This is very similar
to both the Viterbi algorithm and the posterior decoding algorithm. Therefore
the theoretical running time has not been verified as done in the two previous
sections. However, the three terms of the running time are discussed in turn
below.

The first term $M' N^3$ comes from the computation of the $C$ matrices. This
has already been experimented with for the Viterbi algorithm in
section~\ref{sec:theor-runn-times}. Since the only difference between the
computations for the Viterbi algorithm and the forward-backward algorithm has
to do with nummerical stability, the theoretical running will be also hold for
the posterior decoding.

The second term $N^2 T'$ comes from computing the forward and backward tables
for the compressed sequence of length $T'$. In
section~\ref{sec:asympt-runn-time} this was already verified the for posterior
decoding algorithm.

Finally the $N^2 (j - i + \delta)$ term comes from the decompression of the
compressed subsequence that is linear in $(j - i + \delta)$ and from computing
the posterior decoding for the subsequence. The running time of the posterior
decoding has already been shown to match the theoretical running time. The
decompression is quite simple and experiments have not been done for
this. \fxwarning{Make experiment for decompression?}

\subsection{Comparison}

The subsequence posterior decoding algorithm has been compared to the classical
posterior decoding algorithm. For the classical algorithm the entire path as
been found and then the path from index $i$ to $j$ has been extracted.

The running time of the subsequence posterior decoding is much dependent on the
size of the size of the interval for which to compute the path, i.e.\
$j-i$. The value of $\delta$ depends heavily on the data and the
compression. In general, for an increasing compression ratio the value of
$\delta$ will increase. But if a part of a sequence is very compressible and
another part is not compressible, the value of $\delta$ will also depend on the
indices $i$ and $j$.

Depending on how well the data compresses, the value of the length of the
compressed sequence $T'$ will be larger or smaller than the length of
subsequence for which the posterior decoding is returned, that is
$(j - i + \delta)$.  This is the case for e.g.\ random data that only has a
compression ratio of approximately 4 as seen
section~\ref{sec:compression-ratio}. Hence, it is expected that the value of
$i$ and $j$ will not give a major change in the running time. For Fibonacci
words though, the compression ratio is much better and for the experiment made
in section~\ref{sec:compression-ratio} the size of $T'$ becomes 2 for any
$T$. In this case, the value of $i$ and $j$ will be dominant in the running
time.

The comparison of to the classical implementation is split into three
parts. First, the influence of the parameters $i$ and $j$ is measured for a
constant length of the input sequence and a constant number of states. Next the
sequence length is varied for a constant subsequence length, and finally the
number of states is varied.

\subsubsection{Subsequence Length}

For this experiment the length of the substring has been varied for an input
sequences of length 10000 and a model with 16 states. For the experiment using
random sequences 20 different sequences has been used, while for the Fibonacci
word only a single string of a particular length exist.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
