% chktex-file 24
% chktex-file 44

\chapter{Background}
\label{cha:background}

In this chapter preliminary theory of hidden Markov models is described. Also,
the work made by \citet{lifshits2009speeding} and \citet{sand2013ziphmmlib} is
described.

\section{Hidden Markov Models}
\label{sec:hidden-markov-models}

Hidden Markov models, from now on abbreviated HMMs, were first introduced and
studied in the end of the 1960s and the beginning of the 1970s. Since then they
have become very popular and have proven to be both effective and efficient in
many applications. \citep{rabiner1989tutorial}

Think of a system that can be described as a being in one of $N$ states and any
time. Let the set states be notated as $\mathcal{H} = {h_1, h_2, \dots, h_N}$.
In figure~\ref{fig:markov-chain} an example with three states is shown. In some
discrete time steps denoted $t = 1, 2, \dots$ the system changes state to a new
state or back to the same state. For each state there is a certain probability
of making a transition to any other state. This may be represented as a matrix
$A = {\{a_{ij}\}}_{1 \le i \le N}$ with $a_{ij} \in [0, 1]$ and
$\sum_{j = 1}^N a_{ij} = 1$. This is called a Markov process, or more precisely
for discrete-time and finite state spaces: a Markov chain. The non-zero
transition probabilities is shown as arrows in
figure~\ref{fig:markov-chain}. The initial state, at time $t = 1$, is
determined from a vector $\Pi = {(\pi_i)}_{1 \le i \le N}$ with
$\sum_{i=1}^N \pi_i = 1$ being the probability of starting in each state.

\begin{figure}
  \centering
  \input{figures/markov_chain.tex}
  \caption{An example of a Markov chain.}
  \label{fig:markov-chain}
\end{figure}

Having established the above system with $A$ and $\Pi$ given, one may ask
questions like what the probability of observing a certain sequence of states
$I_{1:T}$ during time steps $t = 1, \dots\ T$ is. As an example lets assume the
model in figure~\ref{fig:markov-chain} is a model of the weather with $h_1$
being rainy, $h_2$ being cloudy, and $h_3$ being sunny weather. The probability
of a sequence of observations $I_{1:4} = i_1i_2i_3i_4$ with
$i_i \in \mathcal{H}$ over four days, $t = 1, 2, 3, 4$, may easily be computed
by multiplying the initial probability to the transition probabilities.

The above formality is too limited for many applications, since the states do
not correspond to the actual observable events. To overcome this problem a
distinction between observables and states is made. The observables is a
probabilistic function of the state. Given
$\mathcal{O} = {o_1, o_2, \dots, o_M}$ being a set of $M$ observables an
emission matrix $B = {\{b_{ij}\}}_{1 \le i \le N}^{1 \le j \le M}$ with
$\sum_{j=1}^M b_{ij} = 1$ is defined, where $b_{ij}$ is the probability of
observing $o_j$ when the system is in state $h_i$. The set of states,
$\mathcal{H}$, may now be called ``hidden'' states since they are not
observable. An example of an HMM is shown in
figure~\ref{fig:hidden-markov-model}. The figure is similar to
figure~\ref{fig:markov-chain} except for the probabilities added for each
hidden state.

\begin{figure}
  \centering
  \input{figures/HMM.tex}
  \caption{An example of a hidden Markov model.}
  \label{fig:hidden-markov-model}
\end{figure}

For modelling weather we can now use the fact that the observables (rainy,
cloudy, and sunny weather) depend on the air preasure. This may be modeled as
$h_1$ being low preasure, $h_2$ being high preasure, $o_1$ being rainy, $o_2$
being cloudy, and $o_3$ being sunny weather. As before the probability of a
sequence of observations $Y_{1:4} = y_1y_2y_3y_4$ with $y_i \in \mathcal{O}$
can be found, but it is a harder than before, since an observation sequence may be
generated in many ways.

\citet{rabiner1989tutorial} states three key problems for using hidden Markov
models in real world applications, with the first one being the one just
mentioned.
\begin{enumerate}
\item Given the observation sequence $Y_{1:T} = y_1y_2\dots{}y_T$, and the
  model $\lambda = (A, B, \Pi)$, how we compute $\Pr(O \mid \lambda)$, the
  probability of the observation sequence.
\item Given the observation sequence $Y_{1:T} = y_1y_2\dots{}y_T$, how do we
  choose a state sequence $I = i_1i_2\dots{}i_T$ which is optimal in some
  meaningful sense.
\item How we adjust the model parameters $\lambda = (A, B, \Pi)$ to maximize
  $\Pr(O \mid \lambda)$.
\end{enumerate}

The first problem of finding to joint probability of an observation sequence
$Y_{1:T}$ is solved by the Forward algorithm. This was implemented in zipHMMlib
by \citet{sand2013ziphmmlib}.

For solving the second problem, many different methods may exist as it depends
on how the optimality criterion is set. However, two widely used methods exist.
One solution is to look at all possible sequences of states emitting the
observation sequence and pick the most likely one. This problem is solved
efficiently by the Viterbi algorithm. Another method is to choose the states
$i_t$ that are individually most likely. In this way the expected number of
correct states is maximized, but the state sequence may not be valid, since the
transition probability from $i_t$ to $i_{t + 1}$ might be zero. This is also
called the posterior decoding and is solved by using the Forward-Backward
algorithm.

Different solutions also exist to the third problem. One solution is Baum-Welch
training that is an iterative procedure that maximizes the probability of the
observation sequence by iteratively reestimating the parameters such that the
new model is more likely to generate the observation sequence than the previous
model.

\section{Related Work}

\citet{lifshits2009speeding} present a method for speeding up the dynamic
programming algorithms used with HMMs, namely the forward-backward algorithms
and the Viterbi algorithm. The approach is based on finding repeated substrings
in the observation input sequence. These substrings are found using five different
algorithms: Four Russians method, run length encoding, Lempel-Ziv parsing,
grammer-based compression and byte pair encoding. The forward-backward and the
Viterbi algorithms are the reformulated into series of matrix
multiplications. The overall idea is that the repeated substrings correspond to
repeated matrix multiplications and by finding the repeated substrings the
multiplications can be avoided. The article is primarily theoretical. A single
experiment have been performed on some DNA sequences showing a speed up of the
Viterbi algorithm when using an improved Lempel-Ziv parsing without
backtracking. No experiments have been made for the other algorithms and no
code has been made available.

\citet{sand2013ziphmmlib} present zipHMMlib, a highly optimized HMM library for
speeding up the forward algorithm. The speed up is achieved by finding repeated
substrings using byte pair encoding. Much of the theory in this paper rely on
\cite{lifshits2009speeding}, but it is extended to make the computations
numerically stable. Furthermore, the code is available as an open source
library with bindings for both Python and R.

This thesis extends the work made by \citet{sand2013ziphmmlib} to also include
a highly efficient Viterbi algorithm and a posterior decoding algorithm based
on byte pair encoding as developed theoretically by
\citet{lifshits2009speeding}. The theory will also be extended a bit to make
the computation numerically stable and the experiments will be more extensive.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
