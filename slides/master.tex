\documentclass[english,notes]{beamer}

% Math
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\allowdisplaybreaks{}           % Allow pagebreaks in math environments.
\makeatletter
\g@addto@macro\bfseries{\boldmath} % Typeset bold math in headings.
\makeatother

% Font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{newpxtext}
\usepackage{newpxmath}
\usepackage{microtype}

% Captions
\usepackage[small,hang,bf,margin=30pt]{caption}

% Graphics
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{tikz}
\usepackage{pgfplots}
% \usetikzlibrary{external}
% \tikzexternalize[prefix=tikz/,mode=list and make]
% \pgfkeys{/pgf/images/include external/.code=\includegraphics{#1}} % Makes
                                                                  % externalize
                                                                  % and draft
                                                                  % work
                                                                  % together.
\pgfplotsset{%
  invoke before crossref tikzpicture={\tikzexternaldisable},
  invoke after crossref tikzpicture={\tikzexternalenable},
  ylabel near ticks,
  legend columns=-1,
  legend style={at={(0.5, 1)},anchor=south,draw=none,fill=none,/tikz/every even
    column/.append style={column sep=10pt}},
  every y tick scale label/.style={at={(0,1)},anchor=east, font=\scriptsize,
    inner xsep=2pt, inner ysep=0pt},
  ytick placement tolerance=-2mm,
  max space between ticks=30pt,
}
\usetikzlibrary{arrows}
\usepgfplotslibrary{groupplots}

% Fixme
\usepackage[draft]{fixme}
\fxsetup{inlineface=\bfseries\tiny,inline=true,nomargin}

% Colors
\usepackage{color}
\definecolor{my-red}{HTML}{E41A1C}
\definecolor{my-green}{HTML}{4DAF4A}
\definecolor{my-blue}{HTML}{377EB8}
\definecolor{my-purple}{HTML}{984EA3}
\definecolor{my-orange}{HTML}{FF7F00}

% User defined macros
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% Date
\usepackage{datetime}

% Beamer stuff.
\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[]
{
 \begin{frame}<beamer>
 \frametitle{Agenda}
 \tableofcontents[currentsection]
 \end{frame}
}
\usefonttheme{serif}

% Symbols
\usepackage[weather]{ifsym}
\newcommand{\sun}{\text{\Sun}}
\newcommand{\cloud}{\text{\Cloud}}
\newcommand{\rain}{\text{\Rain}}
\newcommand{\suncloud}{\text{\SunCloud}}

\title{Speeding Up HMM Decoding Using Compression}
\author{Torben Muldvang Andersen, 20093713}
\date{\protect\formatdate{24}{06}{2015}}

\begin{document}
\begin{frame}
  \maketitle
  \includegraphics[height=7mm, trim=0 0 40mm 0, clip]{../logo}
  \hfill
  \includegraphics[height=7mm]{../BiRC-logo}
\end{frame}

\begin{frame}
  \frametitle{Agenda}
    Med udgangspunkt i specialerapporten ønskes et foredrag, hvor de
    væsentligste resultater af specialeprojektet fremlægges.
  \begin{itemize}
  \item Du bedes presentere hvorledes skjulte Markov modeller anvendes til
    Viterbi- og Posterior-decoding med fokus på hvorledes implementeringen af
    disse decoding-methoder kan formuleres ved hjælp af lineær algebra samt
    hvorledes en præprocessesing med henblik på en ``komprimering'' af input
    kan forbedre deres kørselstid i praksis.
  \item Du bedes også præsentere dine implementeringer af metoderne og
    udvidelser til biblioteket zipHMMlib samt de eksperimenter som du har
    gennemført for at undersøge deres kørselstider i praksis, herunder hvornår
    præprocesseringen giver den ønskede forbedring af kørselstiden.
  \item Endelig bedes du overveje hvorvidt andre former for præprocessering
    (komprimering) kunne anvedes.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Agenda}
  \tableofcontents{}
\end{frame}

\section{Goal}

\begin{frame}
  \frametitle{\insertsection}
  Complement the work made by Sand et al. (2013).
  \begin{itemize}
  \item Decoding algorithms: Viterbi and posterior decoding.
  \item Theory, implementation, experiments.
  \end{itemize}
\end{frame}

\section{Hidden Markov Models}

\begin{frame}
  \frametitle{\insertsection}
  \begin{columns}
    \onslide<+->
      \begin{column}{0.5\textwidth}
        \centering
        \input{figures/HMM.tex}
      \end{column}%
    \onslide<+->{
      \begin{column}{0.5\textwidth}
        \begin{tabular}{rl}
          Hidden states                 & L H                                                                                    \\
          Observables                   & \Sun{} \Cloud{} \Rain{}                                                                \\
        \end{tabular}
      \end{column}}
  \end{columns}
  \onslide<+->{
    \begin{center}
      \begin{tabular}{rcccccccccc}
        Observation $Y_{1:T}$           & \Sun{} & \Sun{} & \Cloud{} & \Rain{} & \Sun{} & \Cloud{} & \Sun{} & \Cloud{} & \Rain{} \\
        \onslide<+-> Decoding $X_{1:T}$ & H      & H      & H        & L       & L      & H        & H      & H        & H       \\
      \end{tabular}
    \end{center}}
\end{frame}

\section{Viterbi Decoding}

\subsection{The Original Viterbi Algorithm}
\begin{frame}
  \frametitle{\insertsubsection}
  \onslide<+->
  Given a model $\lambda$ and an observation sequence $Y_{1:T}$, define
  \begin{equation*}
    \delta_t(x_t) = \max_{x_{1:t}} \Pr(Y_{1:t}, X_{1:t} = x_{1:t} \mid \lambda).
  \end{equation*}

  \onslide<+->
  This can be solved by dynamic programming using
  \begin{equation*}
    \begin{aligned}
      \delta_1(x_1) &= \pi_{x_1} b_{x_1, y_1} \\
      \delta_t(x_t) &= \max_{x_{t - 1}} \delta_{t - 1}(x_{t - 1}) a_{x_{t - 1}, x_t} b_{x_t, y_t} .
    \end{aligned}
  \end{equation*}

  \onslide<+->
  \begin{center}
    \input{figures/viterbi.tex}
  \end{center}

  \onslide<+->{
    Backtracking using argmax table to obtain $V_{1:T}$.
  }

  \onslide<8->{
    Running time: $O(N^2 T)$
  }

 \fxwarning{Forklar øverste notation.}
 \fxwarning{Forbered udledning af Viterbi.}
\end{frame}

\subsection{Linear Algebra}

\begin{frame}
  \frametitle{\insertsubsection}
  \onslide<+->{
    \begin{equation*}
      \Pi =
      \begin{bmatrix}
        \pi_1                            \\
        \vdots                           \\
        \pi_N                            \\
      \end{bmatrix}\quad
      A =
      \begin{bmatrix}
        a_{1, 1}   & \cdots & a_{1,N}    \\
        \vdots     & \ddots & \vdots     \\
        a_{N, 1}   & \cdots & a_{N,N}    \\
      \end{bmatrix}\quad
      B_{o_i} =
      \begin{bmatrix}
        b_{1, o_i} &        &            \\
        & \ddots &            \\
        &        & b_{N, o_i} \\
      \end{bmatrix}
    \end{equation*}
  }

  \onslide<+->{
    \begin{equation*}
      \delta_t(x_t) = \max_{x_{t - 1}} \delta_{t - 1}(x_{t - 1}) \underbrace{a_{x_{t - 1},x_t} b_{x_t, y_t}}_{\textstyle C_{o_i} = B_{o_i} A^T}
    \end{equation*}
  }

  \onslide<+->{
    \begin{equation*}
      \begin{aligned}
        \delta_1 &= C_1 = B_{y_1} \Pi\\
        \onslide<+-> \delta_t &= C_{y_t} \odot \delta_{t - 1} = C_{y_t} \odot C_{y_{t-1}} \odot \dots \odot C_1
      \end{aligned}
    \end{equation*}
  }

  \onslide<+->
  Running time: $O(M N^3 + N^2 T)$

  \vfill
  \fxwarning{Reformulation. (Would be really cool if this could be made using an
    illustration of the stuff multiplied in the original algorithm and the
    table using the weather example.)}
  \fxwarning{In the implementation the matrixmultiplications involving
    $B_{o_i}$ takes time $O(N^2)$ since $B$ is stored as an array.}
\end{frame}

\subsection{Exploiting Repetitions}

\begin{frame}
  \frametitle{\insertsubsection}
  Example
  \begin{center}
    \def\tabcolsep{1pt}
    \begin{tabular}{rccccccccccccccccc}
      \onslide<1->{$\delta_{T} =$ & $C_{\rain}$ & $\odot$ & \alert<2->{$C_{\cloud}$} & \alert<2->{$\odot$} & \alert<2->{$C_{\sun}$} & $\odot$ & \alert<2->{$C_{\cloud}$} & \alert<2->{$\odot$} & \alert<2->{$C_{\sun}$} & $\odot$ & $C_{\rain}$ & $\odot$ & \alert<2->{$C_{\cloud}$} & \alert<2->{$\odot$} & \alert<2->{$C_{\sun}$} & $\odot$ & $C_1$ \\}%
      \onslide<3->{$ =$  & $C_{\rain}$ & $\odot$ & \multicolumn{3}{c}{$C_{\suncloud}$} & $\odot$ & \multicolumn{3}{c}{$C_{\suncloud}$} & $\odot$ & $C_{\rain}$ & $\odot$ & \multicolumn{3}{c}{$C_{\suncloud}$} & $\odot$ & $C_1$ \\}%
    \end{tabular}
  \end{center}
  \onslide<4->{
    Byte-pair encoding
  }
  \begin{center}
    \def\tabcolsep{1pt}
    \begin{tabular}{ccccccccccccccccc}
    \onslide<5->{$Y_{1:T} = $ & \Sun{} & \alert<6->{\Sun{}} & \alert<6->{\Cloud{}} & \Rain{} & \alert<6->{\Sun{}} & \alert<6->{\Cloud{}} & \alert<6->{\Sun{}} & \alert<6->{\Cloud{}} & \Rain{} \\}%
    \onslide<7->{& \Sun{} & \multicolumn{2}{c}{\alert<8->{\SunCloud{}}} & \alert<8->{\Rain{}} & \multicolumn{2}{c}{\SunCloud{}} & \multicolumn{2}{c}{\alert<8->{\SunCloud{}}} & \alert<8->{\Rain{}} \\}%
    \onslide<9-> $Y_{1:T'}' = $ & \Sun{} & & \Lightning{} & & \multicolumn{2}{c}{\SunCloud{}} & & \Lightning{} & \\
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Backtracking}
  \onslide<1->{
    From argmax table $V_{1:T'}'$ can be obtained.
    \begin{center}
      \input{figures/viterbi_comp_backtrack.tex}
    \end{center}
  }

  \onslide<2->{
    Compute another set of matrices.
    \begin{equation*}
      C_{\suncloud} = C_{\cloud} \odot C_{\sun}, \quad
      R_{\suncloud}(i, j) = \argmax_k
      \left(
        C_{\cloud}(i, k) \cdot C_{\sun}(k, j)
      \right).
    \end{equation*}
  }

  \onslide<4->{
    Running time: $O(M' N^3 + N^2 T')$ or $O(M' N^3 + N^2 T' + T)$.
  }
\end{frame}
\section{Posterior Decoding}

\begin{frame}
  \frametitle{\insertsection}
  \onslide<+->
  Given a model $\lambda$ and an observation sequence $Y_{1:T}$,
  find the most likely state at the time the symbol is emitted.
  \begin{equation*}
    \begin{aligned}
      p_t & = \argmax_{x_t \in \mathcal{H}} \Pr \left(x_t | Y_{1:T}, \lambda \right) \\
          \onslide<+-> & = \argmax_{x_t \in \mathcal{H}} \alpha(x_t) \beta(x_t).
    \end{aligned}
  \end{equation*}
  \onslide<+->
  Problems in exploiting repetitions:
  \begin{itemize}
  \item<+-> No dependence between sequential states in posterior decoding.
  \item<+-> Computation of full tables cannot be sped up.
  \end{itemize}
\end{frame}

\section{Indexed Posterior Decoding}

\begin{frame}
  \frametitle{\insertsection}
  \onslide<+->
  Given a model $\lambda$ and an observation sequence $Y_{1:T}$, find the
  posterior decoding $P_{i:j}$.

  \begin{itemize}
    \tiny
  \item Definition.
  \item Illustration using figure from thesis.
  \end{itemize}
\end{frame}

\section{Implementation}

\begin{frame}
  \frametitle{\insertsection}
  \begin{itemize}
    \tiny
  \item Something about the implementation.
  \end{itemize}
\end{frame}

\section{Experiments}

\begin{frame}
  \frametitle{\insertsection}
  \begin{itemize}
    \tiny
  \item Main experiments.
  \item Some kind of summary of experiments
  \end{itemize}
\end{frame}

\section{Other Types of Preprocessing}

\begin{frame}
  \frametitle{\insertsection}

\end{frame}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
